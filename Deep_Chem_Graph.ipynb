{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2543de0-f7cf-43d9-9964-4a9e2d46e0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good to go!\n"
     ]
    }
   ],
   "source": [
    "print('good to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb6f161-17f7-4782-aabf-f92f19e61416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Using cached pip-24.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-24.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.1\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (2023.6.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install fsspec\n",
    "!pip install pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c121d8-f3c4-4c06-8832-729fa2984e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.12.1\n",
      "Uninstalling tensorflow-2.12.1:\n",
      "  Successfully uninstalled tensorflow-2.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b81cd1a-17b6-473d-ba95-9f64a903c19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.12)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.54.3)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow)\n",
      "  Using cached keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow)\n",
      "  Using cached optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "Using cached keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "Installing collected packages: namex, libclang, tensorflow-io-gcs-filesystem, optree, ml-dtypes, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.0\n",
      "    Uninstalling ml-dtypes-0.4.0:\n",
      "      Successfully uninstalled ml-dtypes-0.4.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.3\n",
      "    Uninstalling tensorboard-2.12.3:\n",
      "      Successfully uninstalled tensorboard-2.12.3\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "Successfully installed keras-3.3.3 libclang-18.1.1 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c443b352-3457-48f9-8c52-ff1fe7b8ace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.1)\n",
      "Requirement already satisfied: install in /opt/conda/lib/python3.10/site-packages (1.3.5)\n",
      "Requirement already satisfied: tensorflow-probability in /opt/conda/lib/python3.10/site-packages (0.24.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (2.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (1.26.4)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (2.2.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (0.4.0)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-probability) (0.1.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install pip install --upgrade tensorflow-probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e8471b-495a-43ca-b6dd-d7f2afd93f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_parquet_to_pandas_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Reads a Parquet file into a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the Parquet file.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The loaded Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d56d1688-fe6c-45e9-abe1-f93d8bc20f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BRD4 = read_parquet_to_pandas_dataframe('df_BRD4.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4d431dc-c755-45ff-b889-28e77e0bbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10_000\n",
    "\n",
    "# Sampleof the dataset\n",
    "df_BRD4 = df_BRD4[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "294d7dc9-6a84-4063-bfe1-847118590b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_smiles</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256901121</td>\n",
       "      <td>O=C(C[C@@H](Nc1nc(NCC(O)c2ccncc2)nc(Nc2cc(N3CC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>256901127</td>\n",
       "      <td>O=C(C[C@@H](Nc1nc(NCC2(c3ccncc3)CC2)nc(Nc2cc(N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256901130</td>\n",
       "      <td>NC(=O)N1CCC(CNc2nc(Nc3cc(N4CCNCC4)ccc3[N+](=O)...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>256901133</td>\n",
       "      <td>O=C(C[C@@H](Nc1nc(NCC2CCN(CC(F)F)CC2)nc(Nc2cc(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>87031824</td>\n",
       "      <td>O=C(N[Dy])[C@H](Nc1nc(Nc2ccc3ncccc3c2)nc(Nc2n[...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  \\\n",
       "__null_dask_index__              \n",
       "1                    256901121   \n",
       "7                    256901127   \n",
       "10                   256901130   \n",
       "13                   256901133   \n",
       "16                    87031824   \n",
       "\n",
       "                                                       molecule_smiles  binds  \n",
       "__null_dask_index__                                                            \n",
       "1                    O=C(C[C@@H](Nc1nc(NCC(O)c2ccncc2)nc(Nc2cc(N3CC...      1  \n",
       "7                    O=C(C[C@@H](Nc1nc(NCC2(c3ccncc3)CC2)nc(Nc2cc(N...      1  \n",
       "10                   NC(=O)N1CCC(CNc2nc(Nc3cc(N4CCNCC4)ccc3[N+](=O)...      1  \n",
       "13                   O=C(C[C@@H](Nc1nc(NCC2CCN(CC(F)F)CC2)nc(Nc2cc(...      1  \n",
       "16                   O=C(N[Dy])[C@H](Nc1nc(Nc2ccc3ncccc3c2)nc(Nc2n[...      1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BRD4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73c5d4-46cd-4639-853b-ea096acbc7a3",
   "metadata": {},
   "source": [
    "## Running Deep Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb8b0c75-a7dd-4499-acdd-3864ff54c37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepchem in /opt/conda/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from deepchem) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.4.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.12)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.11.4)\n",
      "Requirement already satisfied: rdkit in /opt/conda/lib/python3.10/site-packages (from deepchem) (2024.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit->deepchem) (9.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->deepchem) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->deepchem) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68960766-060b-4d5c-aab6-641f2c3e973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.models import WeaveModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c1bf3a1-feed-426b-ba55-88ba42a98d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize SMILES strings\n",
    "featurizer = dc.feat.WeaveFeaturizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09986a0c-6c1f-44d2-a54c-162e9dc7f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df_BRD4['binds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8b57f-6d48-4772-ba84-baf5350f9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data using CSVLoader with MATFeaturizer\n",
    "import time\n",
    "# Measure encoding speed\n",
    "start_time = time.time()\n",
    "X = featurizer.featurize(df_BRD4['molecule_smiles'])\n",
    "end_time = time.time()\n",
    "\n",
    "num_smiles = len(df_BRD4)\n",
    "encoding_time = end_time - start_time\n",
    "encoding_speed = num_smiles / encoding_time\n",
    "\n",
    "print(f\"Encoded {num_smiles} SMILES strings in {encoding_time:.2f} seconds.\")\n",
    "print(f\"Encoding speed: {encoding_speed:.2f} SMILES strings per second.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "450fca59-a65f-40fb-89e6-30f37f77e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# # Specify the filename\n",
    "# filename = 'X_weave_BRD4.pkl'\n",
    "\n",
    "# # Open the file in binary write mode and use pickle to save the object\n",
    "# with open(filename, 'wb') as file:\n",
    "#     pickle.dump(X, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "481b07ee-11d6-4ac1-b877-7f7f0618f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the filename\n",
    "# filename = 'X_weave_BRD4.pkl'\n",
    "\n",
    "# # Open the file in binary read mode and use pickle to load the object\n",
    "# with open(filename, 'rb') as file:\n",
    "#     loaded_weave_mol_array = pickle.load(file)\n",
    "\n",
    "# # Verify the loaded object\n",
    "# print(loaded_weave_mol_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0497d-d376-44aa-87b8-7588d77d272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DeepChem dataset\n",
    "# dataset = dc.data.NumpyDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d737817-c293-4293-a393-188915b5c1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2edf7f-1001-4409-8daf-c684faf9090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, valid_dataset = splitter.train_test_split(dataset, frac_train=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316a026-8e5e-467b-b039-b00009135e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Graph Convolutional Network (GCN) model\n",
    "model = dc.models.WeaveModel(n_tasks=1, mode=\"classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d3caa-561e-478b-8fb3-e23a348c0afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5741c8-6927-43aa-8add-3ade89aabe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop with verbosity\n",
    "nb_epoch = 10\n",
    "for epoch in range(nb_epoch):\n",
    "    loss = model.fit(train_dataset)\n",
    "    \n",
    "    # Check class distribution in training and validation datasets\n",
    "    train_labels = train_dataset.y\n",
    "    valid_labels = valid_dataset.y\n",
    "    \n",
    "    if len(set(train_labels)) > 1:\n",
    "        train_score = model.evaluate(train_dataset, [dc.metrics.roc_auc_score])\n",
    "        print(f\"  Train ROC-AUC Score: {train_score['roc_auc_score']}\")\n",
    "    else:\n",
    "        print(f\"  Train ROC-AUC Score: Not defined (only one class present in y_true)\")\n",
    "    \n",
    "    if len(set(valid_labels)) > 1:\n",
    "        valid_score = model.evaluate(valid_dataset, [dc.metrics.roc_auc_score])\n",
    "        print(f\"  Valid ROC-AUC Score: {valid_score['roc_auc_score']}\")\n",
    "    else:\n",
    "        print(f\"  Valid ROC-AUC Score: Not defined (only one class present in y_true)\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{nb_epoch}\")\n",
    "    print(f\"  Training Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10634acc-a919-4280-a707-b1c81343d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed6526-5623-4be1-97e9-51985d44637a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf6da64b-d894-4a5b-8872-2297c09d05b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC Score: 0.984871283063401\n",
      "Valid ROC-AUC Score: 0.9841687417085666\n"
     ]
    }
   ],
   "source": [
    "# # Evaluate the model\n",
    "# metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "# train_score = model.evaluate(train_dataset, [metric])\n",
    "# valid_score = model.evaluate(valid_dataset, [metric])\n",
    "# print(f\"Train ROC-AUC Score: {train_score['roc_auc_score']}\")\n",
    "# print(f\"Valid ROC-AUC Score: {valid_score['roc_auc_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "052de2ac-13e1-4e09-8197-7f3a1922d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC Score: 0.9852204570767154\n",
      "Valid ROC-AUC Score: 0.9844390242272392\n"
     ]
    }
   ],
   "source": [
    "# # Evaluate the model\n",
    "# metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "# train_score = model.evaluate(train_dataset, [metric])\n",
    "# valid_score = model.evaluate(valid_dataset, [metric])\n",
    "# print(f\"Train ROC-AUC Score: {train_score['roc_auc_score']}\")\n",
    "# print(f\"Valid ROC-AUC Score: {valid_score['roc_auc_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6331a39-718f-49dd-8769-1893d73729bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save the model\n",
    "save_dir = 'Weave_model_BRD4_40E'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Save the model\n",
    "model.save_checkpoint(model_dir=save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd286e-9fad-4a76-8d92-b7d28431e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b98269-e386-442e-b9e1-c27cd654b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your S3 Bucket and file key\n",
    "bucket = 'kaggle-leash-bio'\n",
    "test_parquet_key = 'test.parquet'\n",
    "test_parquet_location = f's3://{bucket}/{test_parquet_key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403a34f-c791-44c0-9318-219c0077ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Parquet file\n",
    "df = pd.read_parquet(test_parquet_location, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e4fa7-f67f-437c-a106-b1ccaa83742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for molecules binding with the BRD4 protein\n",
    "df_BRD4_test = df[df['protein_name'] == 'BRD4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0acad-5802-461c-82ea-4d89463f0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce for quick view of the test data\n",
    "df_BRD4_test = df_BRD4_test.sample(frac=0.0001, random_state=42)  # 20% random sample\n",
    "print(f\"New DataFrame size: {df_BRD4_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e94567-a485-4ccb-a10d-df600e63f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99f333-8396-499c-b08d-b00fb65055c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = featurizer.featurize(df_BRD4_test['molecule_smiles'].tolist())\n",
    "\n",
    "# Create DeepChem dataset\n",
    "dataset = dc.data.NumpyDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a8fa2-8b7a-4353-b9e1-32f6c461077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict bindings\n",
    "predictions = model.predict(dataset)\n",
    "\n",
    "# Extract the probability of the positive class (binding)\n",
    "probabilities = predictions[:, 0, 1]  # Assuming the second column corresponds to the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c66db-23e4-4626-b30a-d122b7dc7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resulting DataFrame with 'id' and 'binds' columns\n",
    "result_df = pd.DataFrame({\n",
    "    'id': df_BRD4_test['id'],\n",
    "    'binds': probabilities\n",
    "})\n",
    "# Display the resulting DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711afc1-9c3b-4cfd-9fb5-9c308d4e7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BRD4_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b1eaa-fee1-40f8-9c64-059f45c109f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the resulting DataFrame to a CSV file\n",
    "result_df.to_csv('BRD4_weave_predictions_30E.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36f866-9f65-4940-8317-0a4eb46aab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prediction CSV files\n",
    "she_predictions = pd.read_csv('sEH_predictions_30E.csv')\n",
    "hsa_predictions = pd.read_csv('HSA_predictions_20.csv')\n",
    "brd4_predictions = pd.read_csv('BRD4_predictions_30E.csv')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "all_predictions = pd.concat([she_predictions, hsa_predictions, brd4_predictions])\n",
    "\n",
    "# Sort by the 'id' column\n",
    "all_predictions_sorted = all_predictions.sort_values(by='id')\n",
    "\n",
    "# Save to a new CSV file\n",
    "all_predictions_sorted.to_csv('final_submission_sEH_BRD4_30E.csv', index=False)\n",
    "\n",
    "print(\"final_submission.csv created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f61e8-19b3-4c8d-8377-610a876f7024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "680d9602-0934-401e-beb2-e67afde9647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Sequence as SequenceCollection\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except ModuleNotFoundError:\n",
    "    raise ImportError('These classes require PyTorch to be installed.')\n",
    "from typing import List, Tuple, Iterable, Optional, Callable, Union, Sequence\n",
    "from deepchem.data import Dataset\n",
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.utils.typing import OneOrMany, ActivationFn\n",
    "from deepchem.models.losses import L2Loss, SoftmaxCrossEntropy\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "import deepchem.models.torch_models.layers as torch_layers\n",
    "from deepchem.utils.pytorch_utils import get_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e41b5eef-f5a5-4e9b-ad43-425529892bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "317877f0-1bb6-4723-bb10-89689b54b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Sequence as SequenceCollection\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except ModuleNotFoundError:\n",
    "    raise ImportError('These classes require PyTorch to be installed.')\n",
    "from typing import List, Tuple, Iterable, Optional, Callable, Union, Sequence\n",
    "from deepchem.data import Dataset\n",
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.utils.typing import OneOrMany, ActivationFn\n",
    "from deepchem.models.losses import L2Loss, SoftmaxCrossEntropy\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "import deepchem.models.torch_models.layers as torch_layers\n",
    "from deepchem.utils.pytorch_utils import get_activation\n",
    "\n",
    "\n",
    "class Weave(nn.Module):\n",
    "    \"\"\"\n",
    "    A graph convolutional network(GCN) for either classification or regression.\n",
    "    The network consists of the following sequence of layers:\n",
    "\n",
    "    - Weave feature modules\n",
    "\n",
    "    - Final convolution\n",
    "\n",
    "    - Weave Gather Layer\n",
    "\n",
    "    - A fully connected layer\n",
    "\n",
    "    - A Softmax layer\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> import deepchem as dc\n",
    "    >>> featurizer = dc.feat.WeaveFeaturizer()\n",
    "    >>> X = featurizer([\"C\", \"CC\"])\n",
    "    >>> y = np.array([1, 0])\n",
    "    >>> batch_size = 2\n",
    "    >>> weavemodel = dc.models.WeaveModel(n_tasks=1,n_weave=2, fully_connected_layer_sizes=[2000, 1000],mode=\"classification\",batch_size=batch_size)\n",
    "    >>> atom_feat, pair_feat, pair_split, atom_split, atom_to_pair = weavemodel.compute_features_on_batch(X)\n",
    "    >>> model = Weave(n_tasks=1,n_weave=2,fully_connected_layer_sizes=[2000, 1000],mode=\"classification\")\n",
    "    >>> input_data = [atom_feat, pair_feat, pair_split, atom_split, atom_to_pair]\n",
    "    >>> output = model(input_data)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kearnes, Steven, et al. \"Molecular graph convolutions: moving beyond\n",
    "        fingerprints.\" Journal of computer-aided molecular design 30.8 (2016):\n",
    "        595-608.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_tasks: int,\n",
    "        n_atom_feat: OneOrMany[int] = 75,\n",
    "        n_pair_feat: OneOrMany[int] = 14,\n",
    "        n_hidden: int = 50,\n",
    "        n_graph_feat: int = 128,\n",
    "        n_weave: int = 2,\n",
    "        fully_connected_layer_sizes: List[int] = [2000, 100],\n",
    "        conv_weight_init_stddevs: OneOrMany[float] = 0.03,\n",
    "        weight_init_stddevs: OneOrMany[float] = 0.01,\n",
    "        bias_init_consts: OneOrMany[float] = 0.0,\n",
    "        dropouts: OneOrMany[float] = 0.25,\n",
    "        final_conv_activation_fn=F.tanh,\n",
    "        activation_fns: OneOrMany[ActivationFn] = 'relu',\n",
    "        batch_normalize: bool = True,\n",
    "        gaussian_expand: bool = True,\n",
    "        compress_post_gaussian_expansion: bool = False,\n",
    "        mode: str = \"classification\",\n",
    "        n_classes: int = 2,\n",
    "        batch_size: int = 100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_tasks: int\n",
    "            Number of tasks\n",
    "        n_atom_feat: int, optional (default 75)\n",
    "            Number of features per atom. Note this is 75 by default and should be 78\n",
    "            if chirality is used by `WeaveFeaturizer`.\n",
    "        n_pair_feat: int, optional (default 14)\n",
    "            Number of features per pair of atoms.\n",
    "        n_hidden: int, optional (default 50)\n",
    "            Number of units(convolution depths) in corresponding hidden layer\n",
    "        n_graph_feat: int, optional (default 128)\n",
    "            Number of output features for each molecule(graph)\n",
    "        n_weave: int, optional (default 2)\n",
    "            The number of weave layers in this model.\n",
    "        fully_connected_layer_sizes: list (default `[2000, 100]`)\n",
    "            The size of each dense layer in the network.  The length of\n",
    "            this list determines the number of layers.\n",
    "        conv_weight_init_stddevs: list or float (default 0.03)\n",
    "            The standard deviation of the distribution to use for weight\n",
    "            initialization of each convolutional layer. The length of this lisst\n",
    "            should equal `n_weave`. Alternatively, this may be a single value instead\n",
    "            of a list, in which case the same value is used for each layer.\n",
    "        weight_init_stddevs: list or float (default 0.01)\n",
    "            The standard deviation of the distribution to use for weight\n",
    "            initialization of each fully connected layer.  The length of this list\n",
    "            should equal len(layer_sizes).  Alternatively this may be a single value\n",
    "            instead of a list, in which case the same value is used for every layer.\n",
    "        bias_init_consts: list or float (default 0.0)\n",
    "            The value to initialize the biases in each fully connected layer.  The\n",
    "            length of this list should equal len(layer_sizes).\n",
    "            Alternatively this may be a single value instead of a list, in\n",
    "            which case the same value is used for every layer.\n",
    "        dropouts: list or float (default 0.25)\n",
    "            The dropout probablity to use for each fully connected layer.  The length of this list\n",
    "            should equal len(layer_sizes).  Alternatively this may be a single value\n",
    "            instead of a list, in which case the same value is used for every layer.\n",
    "        final_conv_activation_fn: Optional[ActivationFn] (default `F.tanh`)\n",
    "            The activation funcntion to apply to the final\n",
    "            convolution at the end of the weave convolutions. If `None`, then no\n",
    "            activate is applied (hence linear).\n",
    "        activation_fns: str (default `relu`)\n",
    "            The activation function to apply to each fully connected layer.  The length\n",
    "            of this list should equal len(layer_sizes).  Alternatively this may be a\n",
    "            single value instead of a list, in which case the same value is used for\n",
    "            every layer.\n",
    "        batch_normalize: bool, optional (default True)\n",
    "            If this is turned on, apply batch normalization before applying\n",
    "            activation functions on convolutional and fully connected layers.\n",
    "        gaussian_expand: boolean, optional (default True)\n",
    "            Whether to expand each dimension of atomic features by gaussian\n",
    "            histogram\n",
    "        compress_post_gaussian_expansion: bool, optional (default False)\n",
    "            If True, compress the results of the Gaussian expansion back to the\n",
    "            original dimensions of the input.\n",
    "        mode: str (default \"classification\")\n",
    "            Either \"classification\" or \"regression\" for type of model.\n",
    "        n_classes: int (default 2)\n",
    "            Number of classes to predict (only used in classification mode)\n",
    "        batch_size: int (default 100)\n",
    "            Batch size used by this model for training.\n",
    "        \"\"\"\n",
    "        super(Weave, self).__init__()\n",
    "        if mode not in ['classification', 'regression']:\n",
    "            raise ValueError(\n",
    "                \"mode must be either 'classification' or 'regression'\")\n",
    "\n",
    "        if not isinstance(n_atom_feat, SequenceCollection):\n",
    "            n_atom_feat = [n_atom_feat] * n_weave\n",
    "        if not isinstance(n_pair_feat, SequenceCollection):\n",
    "            n_pair_feat = [n_pair_feat] * n_weave\n",
    "        n_layers = len(fully_connected_layer_sizes)\n",
    "        if not isinstance(conv_weight_init_stddevs, SequenceCollection):\n",
    "            conv_weight_init_stddevs = [conv_weight_init_stddevs] * n_weave\n",
    "        if not isinstance(weight_init_stddevs, SequenceCollection):\n",
    "            weight_init_stddevs = [weight_init_stddevs] * n_layers\n",
    "        if not isinstance(bias_init_consts, SequenceCollection):\n",
    "            bias_init_consts = [bias_init_consts] * n_layers\n",
    "        if not isinstance(dropouts, SequenceCollection):\n",
    "            dropouts = [dropouts] * n_layers\n",
    "        if isinstance(\n",
    "                activation_fns,\n",
    "                str) or not isinstance(activation_fns, SequenceCollection):\n",
    "            activation_fns = [activation_fns] * n_layers\n",
    "\n",
    "        self.n_tasks: int = n_tasks\n",
    "        self.n_atom_feat: OneOrMany[int] = n_atom_feat\n",
    "        self.n_pair_feat: OneOrMany[int] = n_pair_feat\n",
    "        self.n_hidden: int = n_hidden\n",
    "        self.n_graph_feat: int = n_graph_feat\n",
    "        self.mode: str = mode\n",
    "        self.n_classes: int = n_classes\n",
    "        self.n_layers: int = n_layers\n",
    "        self.fully_connected_layer_sizes: List[\n",
    "            int] = fully_connected_layer_sizes\n",
    "        self.weight_init_stddevs: OneOrMany[float] = weight_init_stddevs\n",
    "        self.bias_init_consts: OneOrMany[float] = bias_init_consts\n",
    "        self.dropouts: Sequence[float] = dropouts\n",
    "        self.activation_fns: OneOrMany[ActivationFn] = [\n",
    "            get_activation(i) for i in activation_fns\n",
    "        ]\n",
    "        self.batch_normalize: bool = batch_normalize\n",
    "        self.n_weave: int = n_weave\n",
    "\n",
    "        torch.manual_seed(22)\n",
    "        self.layers: nn.ModuleList = nn.ModuleList()\n",
    "        for ind in range(n_weave):\n",
    "            n_atom: int = self.n_atom_feat[ind]\n",
    "            n_pair: int = self.n_pair_feat[ind]\n",
    "            if ind < n_weave - 1:\n",
    "                n_atom_next: int = self.n_atom_feat[ind + 1]\n",
    "                n_pair_next: int = self.n_pair_feat[ind + 1]\n",
    "            else:\n",
    "                n_atom_next = n_hidden\n",
    "                n_pair_next = n_hidden\n",
    "            weave_layer = torch_layers.WeaveLayer(\n",
    "                n_atom_input_feat=n_atom,\n",
    "                n_pair_input_feat=n_pair,\n",
    "                n_atom_output_feat=n_atom_next,\n",
    "                n_pair_output_feat=n_pair_next,\n",
    "                batch_normalize=batch_normalize)\n",
    "            nn.init.trunc_normal_(weave_layer.W_AA,\n",
    "                                  0,\n",
    "                                  std=conv_weight_init_stddevs[ind])\n",
    "            nn.init.trunc_normal_(weave_layer.W_PA,\n",
    "                                  0,\n",
    "                                  std=conv_weight_init_stddevs[ind])\n",
    "            nn.init.trunc_normal_(weave_layer.W_A,\n",
    "                                  0,\n",
    "                                  std=conv_weight_init_stddevs[ind])\n",
    "            if weave_layer.update_pair:\n",
    "                nn.init.trunc_normal_(weave_layer.W_AP,\n",
    "                                      0,\n",
    "                                      std=conv_weight_init_stddevs[ind])\n",
    "                nn.init.trunc_normal_(weave_layer.W_PP,\n",
    "                                      0,\n",
    "                                      std=conv_weight_init_stddevs[ind])\n",
    "                nn.init.trunc_normal_(weave_layer.W_P,\n",
    "                                      0,\n",
    "                                      std=conv_weight_init_stddevs[ind])\n",
    "            self.layers.append(weave_layer)\n",
    "\n",
    "        self.dense1: nn.Linear = nn.Linear(n_hidden, self.n_graph_feat)\n",
    "        self.dense1_act = final_conv_activation_fn\n",
    "        self.dense1_bn: nn.BatchNorm1d = nn.BatchNorm1d(\n",
    "            num_features=self.n_graph_feat,\n",
    "            eps=1e-3,\n",
    "            momentum=0.99,\n",
    "            affine=True,\n",
    "            track_running_stats=True)\n",
    "\n",
    "        self.weave_gather = torch_layers.WeaveGather(\n",
    "            batch_size,\n",
    "            n_input=self.n_graph_feat,\n",
    "            gaussian_expand=gaussian_expand,\n",
    "            compress_post_gaussian_expansion=compress_post_gaussian_expansion)\n",
    "\n",
    "        if n_layers > 0:\n",
    "            self.layers2: nn.ModuleList = nn.ModuleList()\n",
    "            in_size = self.n_graph_feat * 11\n",
    "            for ind, layer_size, weight_stddev, bias_const, dropout, activation_fn in zip(\n",
    "                [0, 1], fully_connected_layer_sizes, weight_init_stddevs,\n",
    "                    bias_init_consts, dropouts, self.activation_fns):\n",
    "                self.layer: nn.Linear = nn.Linear(in_size, layer_size)\n",
    "                nn.init.trunc_normal_(self.layer.weight, 0, std=weight_stddev)\n",
    "                if self.layer.bias is not None:\n",
    "                    self.layer.bias = nn.Parameter(\n",
    "                        torch.full(self.layer.bias.shape, bias_const))\n",
    "                self.layer.layer_bn = nn.BatchNorm1d(num_features=layer_size,\n",
    "                                                     eps=1e-3,\n",
    "                                                     momentum=0.99,\n",
    "                                                     affine=True,\n",
    "                                                     track_running_stats=True)\n",
    "                self.layer.weight_stddev = weight_stddev\n",
    "                self.layer.bias_const = bias_const\n",
    "                self.layer.dropout = nn.Dropout(dropout)\n",
    "                self.layer.layer_act = activation_fn\n",
    "                self.layers2.append(self.layer)\n",
    "                in_size = layer_size\n",
    "\n",
    "        n_tasks = self.n_tasks\n",
    "        if self.mode == 'classification':\n",
    "            n_classes = self.n_classes\n",
    "            self.layer_2 = nn.Linear(fully_connected_layer_sizes[1],\n",
    "                                     n_tasks * n_classes)\n",
    "\n",
    "        else:\n",
    "            self.layer_2 = nn.Linear(fully_connected_layer_sizes[1], n_tasks)\n",
    "\n",
    "    def forward(self, inputs: OneOrMany[torch.Tensor]) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: OneOrMany[torch.Tensor]\n",
    "            Should contain 5 tensors [atom_features, pair_features, pair_split, atom_split, atom_to_pair]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[torch.Tensor]\n",
    "            Output as per use case : regression/classification\n",
    "        \"\"\"\n",
    "        input1: List[np.ndarray] = [\n",
    "            np.array(inputs[0]),\n",
    "            np.array(inputs[1]),\n",
    "            np.array(inputs[2]),\n",
    "            np.array(inputs[4])\n",
    "        ]\n",
    "        for ind in range(self.n_weave):\n",
    "            weave_layer_ind_A, weave_layer_ind_P = self.layers[ind](input1)\n",
    "            input1 = [\n",
    "                weave_layer_ind_A, weave_layer_ind_P,\n",
    "                np.array(inputs[2]),\n",
    "                np.array(inputs[4])\n",
    "            ]\n",
    "\n",
    "        dense1: torch.Tensor = self.dense1(weave_layer_ind_A)\n",
    "        dense1 = self.dense1_act(dense1)\n",
    "        if self.batch_normalize:\n",
    "            self.dense1_bn.eval()\n",
    "            dense1 = self.dense1_bn(dense1)\n",
    "\n",
    "        weave_gather: torch.Tensor = self.weave_gather([dense1, inputs[3]])\n",
    "        if self.n_layers > 0:\n",
    "            input_layer: torch.Tensor = weave_gather\n",
    "            for ind, dropout in zip([0, 1], self.dropouts):\n",
    "                dense2 = self.layers2[ind]\n",
    "                layer = self.layers2[ind](input_layer)\n",
    "                if dropout > 0.0:\n",
    "                    dense2.dropout.eval()\n",
    "                    layer = dense2.dropout(layer)\n",
    "                if self.batch_normalize:\n",
    "                    dense2.layer_bn.eval()\n",
    "                    layer = dense2.layer_bn(layer)\n",
    "                layer = dense2.layer_act(layer)\n",
    "                input_layer = layer\n",
    "            output: torch.Tensor = input_layer\n",
    "        else:\n",
    "            output = weave_gather\n",
    "\n",
    "        n_tasks = self.n_tasks\n",
    "        if self.mode == 'classification':\n",
    "            n_classes = self.n_classes\n",
    "            logits: torch.Tensor = torch.reshape(self.layer_2(output),\n",
    "                                                 (-1, n_tasks, n_classes))\n",
    "            output = F.softmax(logits, dim=2)\n",
    "            outputs: List[torch.Tensor] = [output, logits]\n",
    "        else:\n",
    "            output = self.layer_2(output)\n",
    "            outputs = [output]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class WeaveModel(TorchModel):\n",
    "    \"\"\"Implements Google-style Weave Graph Convolutions\n",
    "\n",
    "    This model implements the Weave style graph convolutions\n",
    "    from [1]_.\n",
    "\n",
    "    The biggest difference between WeaveModel style convolutions\n",
    "    and GraphConvModel style convolutions is that Weave\n",
    "    convolutions model bond features explicitly. This has the\n",
    "    side effect that it needs to construct a NxN matrix\n",
    "    explicitly to model bond interactions. This may cause\n",
    "    scaling issues, but may possibly allow for better modeling\n",
    "    of subtle bond effects.\n",
    "\n",
    "    Note that [1]_ introduces a whole variety of different architectures for\n",
    "    Weave models. The default settings in this class correspond to the W2N2\n",
    "    variant from [1]_ which is the most commonly used variant..\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    Here's an example of how to fit a `WeaveModel` on a tiny sample dataset.\n",
    "\n",
    "    >>> import numpy as np\n",
    "    >>> import deepchem as dc\n",
    "    >>> featurizer = dc.feat.WeaveFeaturizer()\n",
    "    >>> X = featurizer([\"C\", \"CC\"])\n",
    "    >>> y = np.array([1, 0])\n",
    "    >>> dataset = dc.data.NumpyDataset(X, y)\n",
    "    >>> model = dc.models.WeaveModel(n_tasks=1, n_weave=2, fully_connected_layer_sizes=[2000, 1000], mode=\"classification\")\n",
    "    >>> loss = model.fit(dataset)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kearnes, Steven, et al. \"Molecular graph convolutions: moving beyond\n",
    "        fingerprints.\" Journal of computer-aided molecular design 30.8 (2016):\n",
    "        595-608.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 n_atom_feat: OneOrMany[int] = 75,\n",
    "                 n_pair_feat: OneOrMany[int] = 14,\n",
    "                 n_hidden: int = 50,\n",
    "                 n_graph_feat: int = 128,\n",
    "                 n_weave: int = 2,\n",
    "                 fully_connected_layer_sizes: List[int] = [2000, 100],\n",
    "                 conv_weight_init_stddevs: OneOrMany[float] = 0.03,\n",
    "                 weight_init_stddevs: OneOrMany[float] = 0.01,\n",
    "                 bias_init_consts: OneOrMany[float] = 0.0,\n",
    "                 weight_decay_penalty: float = 0.0,\n",
    "                 weight_decay_penalty_type: str = \"l2\",\n",
    "                 dropouts: OneOrMany[float] = 0.25,\n",
    "                 final_conv_activation_fn: Optional[ActivationFn] = F.tanh,\n",
    "                 activation_fns: OneOrMany[ActivationFn] = 'relu',\n",
    "                 batch_normalize: bool = True,\n",
    "                 gaussian_expand: bool = True,\n",
    "                 compress_post_gaussian_expansion: bool = False,\n",
    "                 mode: str = \"classification\",\n",
    "                 n_classes: int = 2,\n",
    "                 batch_size: int = 100,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_tasks: int\n",
    "            Number of tasks\n",
    "        n_atom_feat: int, optional (default 75)\n",
    "            Number of features per atom. Note this is 75 by default and should be 78\n",
    "            if chirality is used by `WeaveFeaturizer`.\n",
    "        n_pair_feat: int, optional (default 14)\n",
    "            Number of features per pair of atoms.\n",
    "        n_hidden: int, optional (default 50)\n",
    "            Number of units(convolution depths) in corresponding hidden layer\n",
    "        n_graph_feat: int, optional (default 128)\n",
    "            Number of output features for each molecule(graph)\n",
    "        n_weave: int, optional (default 2)\n",
    "            The number of weave layers in this model.\n",
    "        fully_connected_layer_sizes: list (default `[2000, 100]`)\n",
    "            The size of each dense layer in the network.  The length of\n",
    "            this list determines the number of layers.\n",
    "        conv_weight_init_stddevs: list or float (default 0.03)\n",
    "            The standard deviation of the distribution to use for weight\n",
    "            initialization of each convolutional layer. The length of this lisst\n",
    "            should equal `n_weave`. Alternatively, this may be a single value instead\n",
    "            of a list, in which case the same value is used for each layer.\n",
    "        weight_init_stddevs: list or float (default 0.01)\n",
    "            The standard deviation of the distribution to use for weight\n",
    "            initialization of each fully connected layer.  The length of this list\n",
    "            should equal len(layer_sizes).  Alternatively this may be a single value\n",
    "            instead of a list, in which case the same value is used for every layer.\n",
    "        bias_init_consts: list or float (default 0.0)\n",
    "            The value to initialize the biases in each fully connected layer.  The\n",
    "            length of this list should equal len(layer_sizes).\n",
    "            Alternatively this may be a single value instead of a list, in\n",
    "            which case the same value is used for every layer.\n",
    "        weight_decay_penalty: float (default 0.0)\n",
    "            The magnitude of the weight decay penalty to use\n",
    "        weight_decay_penalty_type: str (default \"l2\")\n",
    "            The type of penalty to use for weight decay, either 'l1' or 'l2'\n",
    "        dropouts: list or float (default 0.25)\n",
    "            The dropout probablity to use for each fully connected layer.  The length of this list\n",
    "            should equal len(layer_sizes).  Alternatively this may be a single value\n",
    "            instead of a list, in which case the same value is used for every layer.\n",
    "        final_conv_activation_fn: Optional[ActivationFn] (default `F.tanh`)\n",
    "            The activation funcntion to apply to the final\n",
    "            convolution at the end of the weave convolutions. If `None`, then no\n",
    "            activate is applied (hence linear).\n",
    "        activation_fns: str (default `relu`)\n",
    "            The activation function to apply to each fully connected layer.  The length\n",
    "            of this list should equal len(layer_sizes).  Alternatively this may be a\n",
    "            single value instead of a list, in which case the same value is used for\n",
    "            every layer.\n",
    "        batch_normalize: bool, optional (default True)\n",
    "            If this is turned on, apply batch normalization before applying\n",
    "            activation functions on convolutional and fully connected layers.\n",
    "        gaussian_expand: boolean, optional (default True)\n",
    "            Whether to expand each dimension of atomic features by gaussian\n",
    "            histogram\n",
    "        compress_post_gaussian_expansion: bool, optional (default False)\n",
    "            If True, compress the results of the Gaussian expansion back to the\n",
    "            original dimensions of the input.\n",
    "        mode: str (default \"classification\")\n",
    "            Either \"classification\" or \"regression\" for type of model.\n",
    "        n_classes: int (default 2)\n",
    "            Number of classes to predict (only used in classification mode)\n",
    "        batch_size: int (default 100)\n",
    "            Batch size used by this model for training.\n",
    "        \"\"\"\n",
    "        self.mode: str = mode\n",
    "        self.model = Weave(\n",
    "            n_tasks=n_tasks,\n",
    "            n_atom_feat=n_atom_feat,\n",
    "            n_pair_feat=n_pair_feat,\n",
    "            n_hidden=n_hidden,\n",
    "            n_graph_feat=n_graph_feat,\n",
    "            n_weave=n_weave,\n",
    "            fully_connected_layer_sizes=fully_connected_layer_sizes,\n",
    "            conv_weight_init_stddevs=conv_weight_init_stddevs,\n",
    "            weight_init_stddevs=weight_init_stddevs,\n",
    "            bias_init_consts=bias_init_consts,\n",
    "            dropouts=dropouts,\n",
    "            final_conv_activation_fn=final_conv_activation_fn,\n",
    "            activation_fns=activation_fns,\n",
    "            batch_normalize=batch_normalize,\n",
    "            gaussian_expand=gaussian_expand,\n",
    "            compress_post_gaussian_expansion=compress_post_gaussian_expansion,\n",
    "            mode=mode,\n",
    "            n_classes=n_classes,\n",
    "            batch_size=batch_size)\n",
    "\n",
    "        if mode not in ['classification', 'regression']:\n",
    "            raise ValueError(\n",
    "                \"mode must be either 'classification' or 'regression'\")\n",
    "\n",
    "        regularization_loss: Optional[Callable]\n",
    "        if weight_decay_penalty != 0.0:\n",
    "            weights = [layer.weight for layer in self.model.layers2]\n",
    "            if weight_decay_penalty_type == 'l1':\n",
    "                regularization_loss = lambda: weight_decay_penalty * torch.sum(  # noqa: E731\n",
    "                    torch.stack([torch.abs(w).sum() for w in weights]))\n",
    "            else:\n",
    "                regularization_loss = lambda: weight_decay_penalty * torch.sum(  # noqa: E731\n",
    "                    torch.stack([torch.square(w).sum() for w in weights]))\n",
    "        else:\n",
    "            regularization_loss = None\n",
    "\n",
    "        loss: Union[SoftmaxCrossEntropy, L2Loss]\n",
    "\n",
    "        if self.mode == 'classification':\n",
    "            output_types = ['prediction', 'loss']\n",
    "            loss = SoftmaxCrossEntropy()\n",
    "        else:\n",
    "            output_types = ['prediction']\n",
    "            loss = L2Loss()\n",
    "\n",
    "        super(WeaveModel,\n",
    "              self).__init__(self.model,\n",
    "                             loss=loss,\n",
    "                             output_types=output_types,\n",
    "                             batch_size=batch_size,\n",
    "                             regularization_loss=regularization_loss,\n",
    "                             **kwargs)\n",
    "\n",
    "    def compute_features_on_batch(self, X_b):\n",
    "        \"\"\"Compute tensors that will be input into the model from featurized representation.\n",
    "\n",
    "        The featurized input to `WeaveModel` is instances of `WeaveMol` created by\n",
    "        `WeaveFeaturizer`. This method converts input `WeaveMol` objects into\n",
    "        tensors used by the Keras implementation to compute `WeaveModel` outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_b: np.ndarray\n",
    "            A numpy array with dtype=object where elements are `WeaveMol` objects.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        atom_feat: np.ndarray\n",
    "            Of shape `(N_atoms, N_atom_feat)`.\n",
    "        pair_feat: np.ndarray\n",
    "            Of shape `(N_pairs, N_pair_feat)`. Note that `N_pairs` will depend on\n",
    "            the number of pairs being considered. If `max_pair_distance` is\n",
    "            `None`, then this will be `N_atoms**2`. Else it will be the number\n",
    "            of pairs within the specifed graph distance.\n",
    "        pair_split: np.ndarray\n",
    "            Of shape `(N_pairs,)`. The i-th entry in this array will tell you the\n",
    "            originating atom for this pair (the \"source\"). Note that pairs are\n",
    "            symmetric so for a pair `(a, b)`, both `a` and `b` will separately be\n",
    "            sources at different points in this array.\n",
    "        atom_split: np.ndarray\n",
    "            Of shape `(N_atoms,)`. The i-th entry in this array will be the molecule\n",
    "            with the i-th atom belongs to.\n",
    "        atom_to_pair: np.ndarray\n",
    "            Of shape `(N_pairs, 2)`. The i-th row in this array will be the array\n",
    "            `[a, b]` if `(a, b)` is a pair to be considered. (Note by symmetry, this\n",
    "            implies some other row will contain `[b, a]`.\n",
    "        \"\"\"\n",
    "        atom_feat = []\n",
    "        pair_feat = []\n",
    "        atom_split = []\n",
    "        atom_to_pair = []\n",
    "        pair_split = []\n",
    "        start = 0\n",
    "        for im, mol in enumerate(X_b):\n",
    "            n_atoms = mol.get_num_atoms()\n",
    "            # pair_edges is of shape (2, N)\n",
    "            pair_edges = mol.get_pair_edges()\n",
    "            # number of atoms in each molecule\n",
    "            atom_split.extend([im] * n_atoms)\n",
    "            # index of pair features\n",
    "            C0, C1 = np.meshgrid(np.arange(n_atoms), np.arange(n_atoms))\n",
    "            atom_to_pair.append(pair_edges.T + start)\n",
    "            # Get starting pair atoms\n",
    "            pair_starts = pair_edges.T[:, 0]\n",
    "            # number of pairs for each atom\n",
    "            pair_split.extend(pair_starts + start)\n",
    "            start = start + n_atoms\n",
    "\n",
    "            # atom features\n",
    "            atom_feat.append(mol.get_atom_features())\n",
    "            # pair features\n",
    "            pair_feat.append(mol.get_pair_features())\n",
    "\n",
    "        return (np.concatenate(atom_feat, axis=0),\n",
    "                np.concatenate(pair_feat, axis=0), np.array(pair_split),\n",
    "                np.array(atom_split), np.concatenate(atom_to_pair, axis=0))\n",
    "\n",
    "    def default_generator(\n",
    "            self,\n",
    "            dataset: Dataset,\n",
    "            epochs: int = 1,\n",
    "            mode: str = 'fit',\n",
    "            deterministic: bool = True,\n",
    "            pad_batches: bool = True) -> Iterable[Tuple[List, List, List]]:\n",
    "        \"\"\"Convert a dataset into the tensors needed for learning.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: `dc.data.Dataset`\n",
    "            Dataset to convert\n",
    "        epochs: int, optional (Default 1)\n",
    "            Number of times to walk over `dataset`\n",
    "        mode: str, optional (Default 'fit')\n",
    "            Ignored in this implementation.\n",
    "        deterministic: bool, optional (Default True)\n",
    "            Whether the dataset should be walked in a deterministic fashion\n",
    "        pad_batches: bool, optional (Default True)\n",
    "            If true, each returned batch will have size `self.batch_size`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Iterator which walks over the batches\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            for (X_b, y_b, w_b,\n",
    "                 ids_b) in dataset.iterbatches(batch_size=self.batch_size,\n",
    "                                               deterministic=deterministic,\n",
    "                                               pad_batches=pad_batches):\n",
    "                if y_b is not None:\n",
    "                    if self.model.mode == 'classification':\n",
    "                        y_b = to_one_hot(y_b.flatten(),\n",
    "                                         self.model.n_classes).reshape(\n",
    "                                             -1, self.model.n_tasks,\n",
    "                                             self.model.n_classes)\n",
    "                inputs = self.compute_features_on_batch(X_b)\n",
    "                yield (inputs, [y_b], [w_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5849a-bf72-43af-b471-f8a3cc6f80bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125028d-8af4-46c1-b095-a8f6aafaf52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b3735b-dd64-4281-8ec1-9dec814e4058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
