{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c39a386e-b8c1-4616-bef8-3ac5da0a912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepchem in /opt/conda/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from deepchem) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.4.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.12)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.11.4)\n",
      "Requirement already satisfied: rdkit in /opt/conda/lib/python3.10/site-packages (from deepchem) (2023.9.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit->deepchem) (9.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->deepchem) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->deepchem) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0.post200)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install deepchem\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cd4c68-bc4a-4a16-b318-1fb9601daf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "2024-06-19 06:38:59.751635: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/opt/conda/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import deepchem as dc\n",
    "from deepchem.models.torch_models import MATModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4d49806-4a8e-4d81-ae79-c25ee5ca14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_data(df, n_jobs=-1):\n",
    "    smiles = df['molecule_smiles'].tolist()\n",
    "    X = featurizer.featurize(smiles)\n",
    "    y = df['binds'].tolist()\n",
    "    dataset = dc.data.NumpyDataset(X, y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51dfffe-79a8-4655-b728-ac1f32a71887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e61fdef-1abd-40a8-a769-34a74789b224",
   "metadata": {},
   "source": [
    "## Quick Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef608be-e180-4259-b231-e0920bbd3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize SMILES strings using ConvMolFeaturizer for MAT\n",
    "featurizer = dc.feat.MATFeaturizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91f506b6-450a-4600-ae36-6c994d13a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_parquet_to_pandas_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Reads a Parquet file into a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the Parquet file.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The loaded Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a2ecacc-d9a8-4b39-b540-141246e999fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HSA = read_parquet_to_pandas_dataframe('df_HSA.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0c4e9f4-7144-4ebd-9bfb-96c9e9f42ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame size: (26, 3)\n"
     ]
    }
   ],
   "source": [
    "# # Assuming df_HSA is your DataFrame\n",
    "df_HSA = df_HSA.sample(frac=0.2, random_state=42)  # 20% random sample\n",
    "\n",
    "print(f\"New DataFrame size: {df_HSA.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09a60660-d53c-445f-bb05-48b9d8e5cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "\n",
    "# Suppress RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3eafb4a-8db7-4586-9b2d-23e19984d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_HSA = featurize_data(df_HSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3ff69ee-6d68-4027-ae87-735d0e88d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_dataset_HSA, valid_dataset_HSA = splitter.train_test_split(train_dataset_HSA, frac_train=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a909a8-2c3e-473f-b55e-a9732b13f271",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ffcef5d-1612-458c-aa4c-8ae9c71708a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(dataset_train, dataset_valid, save_dir):\n",
    "    model = MATModel(\n",
    "        mode='classification',\n",
    "        n_tasks=1,\n",
    "        number_of_layers=6,  # Number of Transformer layers\n",
    "        d_model=256,         # Dimensionality of model embeddings\n",
    "        num_heads=8,         # Number of attention heads\n",
    "        dropout=0.1,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Training with early stopping and learning rate scheduler\n",
    "    best_valid_score = 0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    initial_lr = 0.001\n",
    "\n",
    "    nb_epoch = 50\n",
    "    for epoch in range(nb_epoch):\n",
    "        loss = model.fit(dataset_train, nb_epoch=1)\n",
    "        \n",
    "        # Update learning rate\n",
    "        new_lr = initial_lr * (0.9 ** (epoch // 10))  # Reduce learning rate every 10 epochs\n",
    "        model.optimizer.learning_rate = new_lr\n",
    "        \n",
    "        train_score = model.evaluate(dataset_train, [dc.metrics.roc_auc_score])\n",
    "        valid_score = model.evaluate(dataset_valid, [dc.metrics.roc_auc_score])\n",
    "        print(f\"Epoch {epoch+1}/{nb_epoch}\")\n",
    "        print(f\"  Training Loss: {loss}\")\n",
    "        print(f\"  Train ROC-AUC Score: {train_score['mean-roc_auc_score']}\")\n",
    "        print(f\"  Valid ROC-AUC Score: {valid_score['mean-roc_auc_score']}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if valid_score['mean-roc_auc_score'] > best_valid_score:\n",
    "            best_valid_score = valid_score['mean-roc_auc_score']\n",
    "            model.save_checkpoint(model_dir=save_dir)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    model.restore(model_dir=save_dir)\n",
    "    print(\"Best model restored.\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1018dde-e561-42ff-9174-9ccf8b279fc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_HSA \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset_HSA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset_HSA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmat_model_HSA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 23\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(dataset_train, dataset_valid, save_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m nb_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_epoch):\n\u001b[0;32m---> 23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     new_lr \u001b[38;5;241m=\u001b[39m initial_lr \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (epoch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m))  \u001b[38;5;66;03m# Reduce learning rate every 10 epochs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/deepchem/models/torch_models/torch_model.py:338\u001b[0m, in \u001b[0;36mTorchModel.fit\u001b[0;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    290\u001b[0m         dataset: Dataset,\n\u001b[1;32m    291\u001b[0m         nb_epoch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m         callbacks: Union[Callable, List[Callable]] \u001b[38;5;241m=\u001b[39m [],\n\u001b[1;32m    299\u001b[0m         all_losses: Optional[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train this model on a dataset.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    The average loss over the most recent checkpoint interval\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_checkpoints_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_losses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/deepchem/models/torch_models/torch_model.py:439\u001b[0m, in \u001b[0;36mTorchModel.fit_generator\u001b[0;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [outputs[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_outputs]\n\u001b[1;32m    438\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m loss(outputs, labels, weights)\n\u001b[0;32m--> 439\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_schedule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "model_HSA = train_and_evaluate(train_dataset_HSA, valid_dataset_HSA, 'mat_model_HSA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51598dc-391b-4c49-af3f-187cf79a6cc3",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f3552-8bb2-4b81-856c-f7323628c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your S3 Bucket and file key\n",
    "bucket = 'kaggle-leash-bio'\n",
    "test_parquet_key = 'test.parquet'\n",
    "test_parquet_location = f's3://{bucket}/{test_parquet_key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd7c6f-4200-433f-a7ec-3dc9052b3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Parquet file\n",
    "df = pd.read_parquet(test_parquet_location, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a6ab3-96cb-4614-ac80-7cb8866309ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for molecules binding with the HSA protein\n",
    "df_HSA_test = df[df['protein_name'] == 'HSA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73219d-e90b-4b42-a224-44b3b631ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce for quick view of the test data\n",
    "df_HSA_test = df_HSA_test.sample(frac=0.001, random_state=42)  # 20% random sample\n",
    "print(f\"New DataFrame size: {df_HSA_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea2307-a24b-4c30-b8a4-3a0590e9633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = featurizer.featurize(df_HSA_test['molecule_smiles'].tolist())\n",
    "\n",
    "# Create DeepChem dataset\n",
    "dataset = dc.data.NumpyDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90be015-2165-4f5c-9d3b-5d5b777d42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict bindings\n",
    "predictions = model.predict(dataset)\n",
    "\n",
    "# Extract the probability of the positive class (binding)\n",
    "probabilities = predictions[:, 0, 1]  # Assuming the second column corresponds to the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a75a975-b35b-44d0-8e93-e986b3a31e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resulting DataFrame with 'id' and 'binds' columns\n",
    "result_df = pd.DataFrame({\n",
    "    'id': df_HSA_test['id'],\n",
    "    'binds': probabilities\n",
    "})\n",
    "# Display the resulting DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6bd94-38f5-4014-ab10-36529c5ef483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the resulting DataFrame to a CSV file\n",
    "result_df.to_csv('MAT_HSA_predictions_50E.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
