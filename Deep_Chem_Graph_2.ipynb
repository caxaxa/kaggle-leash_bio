{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2543de0-f7cf-43d9-9964-4a9e2d46e0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good to go!\n"
     ]
    }
   ],
   "source": [
    "print('good to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae26b8ac-77b4-4cd8-99d3-d9fb1300b20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (2023.6.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install fsspec\n",
    "!pip install pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e8471b-495a-43ca-b6dd-d7f2afd93f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_parquet_to_pandas_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Reads a Parquet file into a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the Parquet file.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The loaded Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56d1688-fe6c-45e9-abe1-f93d8bc20f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HSA = read_parquet_to_pandas_dataframe('df_HSA.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d431dc-c755-45ff-b889-28e77e0bbb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame size: (163, 3)\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_HSA is your DataFrame\n",
    "df_HSA = df_HSA.sample(frac=0.0001, random_state=42)  # 20% random sample\n",
    "\n",
    "print(f\"New DataFrame size: {df_HSA.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73c5d4-46cd-4639-853b-ea096acbc7a3",
   "metadata": {},
   "source": [
    "## Running Deep Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb8b0c75-a7dd-4499-acdd-3864ff54c37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepchem in /opt/conda/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from deepchem) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.4.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.12)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.11.4)\n",
      "Requirement already satisfied: rdkit in /opt/conda/lib/python3.10/site-packages (from deepchem) (2023.9.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit->deepchem) (9.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->deepchem) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->deepchem) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68960766-060b-4d5c-aab6-641f2c3e973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.models import GraphConvModel\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from collections.abc import Sequence as SequenceCollection\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except ModuleNotFoundError:\n",
    "    raise ImportError('These classes require PyTorch to be installed.')\n",
    "from typing import List, Tuple, Iterable, Optional, Callable, Union, Sequence\n",
    "from deepchem.data import Dataset\n",
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.utils.typing import OneOrMany, ActivationFn\n",
    "from deepchem.models.losses import L2Loss, SoftmaxCrossEntropy\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "import deepchem.models.torch_models.layers as torch_layers\n",
    "from deepchem.utils.pytorch_utils import get_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02124185-1d2d-4be7-bdae-3ae2af90c244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1bf3a1-feed-426b-ba55-88ba42a98d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize SMILES strings\n",
    "featurizer = dc.feat.WeaveFeaturizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a8b57f-6d48-4772-ba84-baf5350f9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = featurizer.featurize(df_HSA['molecule_smiles'])\n",
    "y = np.array(df_HSA['binds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87c0497d-d376-44aa-87b8-7588d77d272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DeepChem dataset\n",
    "dataset = dc.data.NumpyDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2edf7f-1001-4409-8daf-c684faf9090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "train_dataset, valid_dataset = splitter.train_test_split(dataset, frac_train=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e26db4af-c32b-448a-866d-2bf5c960297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.models import WeaveModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5316a026-8e5e-467b-b039-b00009135e97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to BatchNormalization: {'renorm': True}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define a Graph Convolutional Network (GCN) model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Define the WeaveModel\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWeaveModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/deepchem/models/graph_models.py:229\u001b[0m, in \u001b[0;36mWeaveModel.__init__\u001b[0;34m(self, n_tasks, n_atom_feat, n_pair_feat, n_hidden, n_graph_feat, n_weave, fully_connected_layer_sizes, conv_weight_init_stddevs, weight_init_stddevs, bias_init_consts, weight_decay_penalty, weight_decay_penalty_type, dropouts, final_conv_activation_fn, activation_fns, batch_normalize, batch_normalize_kwargs, gaussian_expand, compress_post_gaussian_expansion, mode, n_classes, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         n_atom_next \u001b[38;5;241m=\u001b[39m n_hidden\n\u001b[1;32m    228\u001b[0m         n_pair_next \u001b[38;5;241m=\u001b[39m n_hidden\n\u001b[0;32m--> 229\u001b[0m     weave_layer_ind_A, weave_layer_ind_P \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWeaveLayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_atom_input_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_atom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_pair_input_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_atom_output_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_atom_next\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_pair_output_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_pair_next\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTruncatedNormal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstddev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_weight_init_stddevs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_normalize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    238\u001b[0m         weave_layer_ind_A, weave_layer_ind_P, pair_split, atom_to_pair\n\u001b[1;32m    239\u001b[0m     ]\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Final atom-layer convolution. Note this differs slightly from the paper\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# since we use a tanh activation as default. This seems necessary for numerical\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# stability.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/deepchem/models/layers.py:2841\u001b[0m, in \u001b[0;36mWeaveLayer.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   2837\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_AA \u001b[38;5;241m=\u001b[39m init([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_atom_input_feat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_hidden_AA])\n\u001b[1;32m   2838\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_AA \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   2839\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_hidden_AA,\n\u001b[1;32m   2840\u001b[0m ])\n\u001b[0;32m-> 2841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAA_bn \u001b[38;5;241m=\u001b[39m \u001b[43mBatchNormalization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_normalize_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_PA \u001b[38;5;241m=\u001b[39m init([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_pair_input_feat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_hidden_PA])\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_PA \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   2845\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_hidden_PA,\n\u001b[1;32m   2846\u001b[0m ])\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to BatchNormalization: {'renorm': True}"
     ]
    }
   ],
   "source": [
    "# Define a Graph Convolutional Network (GCN) model\n",
    "# Define the WeaveModel\n",
    "model = WeaveModel(\n",
    "    n_tasks=1,\n",
    "    mode='classification',\n",
    "    dropout=0.2,\n",
    "    learning_rate=0.001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2911004e-0f14-4138-b296-a10b74d5c32d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m nb_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_epoch):\n\u001b[0;32m----> 5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     train_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(train_dataset, [dc\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mroc_auc_score])\n\u001b[1;32m      7\u001b[0m     valid_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(valid_dataset, [dc\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mroc_auc_score])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/deepchem/models/torch_models/torch_model.py:338\u001b[0m, in \u001b[0;36mTorchModel.fit\u001b[0;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    290\u001b[0m         dataset: Dataset,\n\u001b[1;32m    291\u001b[0m         nb_epoch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m         callbacks: Union[Callable, List[Callable]] \u001b[38;5;241m=\u001b[39m [],\n\u001b[1;32m    299\u001b[0m         all_losses: Optional[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train this model on a dataset.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    The average loss over the most recent checkpoint interval\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_checkpoints_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_losses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/deepchem/models/torch_models/torch_model.py:433\u001b[0m, in \u001b[0;36mTorchModel.fit_generator\u001b[0;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    430\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    432\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 433\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [outputs]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[21], line 280\u001b[0m, in \u001b[0;36mWeave.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: OneOrMany[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m        Output as per use case : regression/classification\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     input1: List[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 280\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    281\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(inputs[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m    282\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(inputs[\u001b[38;5;241m2\u001b[39m]),\n\u001b[1;32m    283\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(inputs[\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m    284\u001b[0m     ]\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_weave):\n\u001b[1;32m    286\u001b[0m         weave_layer_ind_A, weave_layer_ind_P \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[ind](input1)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "\n",
    "# Fit the model\n",
    "# Custom training loop with verbosity\n",
    "nb_epoch = 30\n",
    "for epoch in range(nb_epoch):\n",
    "    loss = model.fit(train_dataset, nb_epoch=1)\n",
    "    train_score = model.evaluate(train_dataset, [dc.metrics.roc_auc_score])\n",
    "    valid_score = model.evaluate(valid_dataset, [dc.metrics.roc_auc_score])\n",
    "    print(f\"Epoch {epoch+1}/{nb_epoch}\")\n",
    "    print(f\"  Training Loss: {loss}\")\n",
    "    print(f\"  Train ROC-AUC Score: {train_score['metric-1']}\")\n",
    "    print(f\"  Valid ROC-AUC Score: {valid_score['metric-1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81b86f-5677-41e4-8cb5-524b3f93cfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee6ae4-3efd-4d72-af30-e2c89d884dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10634acc-a919-4280-a707-b1c81343d122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  Training Loss: 0.24939088821411132\n",
      "  Train ROC-AUC Score: 0.9514079216206295\n",
      "  Valid ROC-AUC Score: 0.9492388248257698\n",
      "Epoch 2/50\n",
      "  Training Loss: 0.24578557115920047\n",
      "  Train ROC-AUC Score: 0.9517783745251939\n",
      "  Valid ROC-AUC Score: 0.9494798000139928\n",
      "Epoch 3/50\n",
      "  Training Loss: 0.225072979927063\n",
      "  Train ROC-AUC Score: 0.953058426332761\n",
      "  Valid ROC-AUC Score: 0.9506386540980909\n",
      "Epoch 4/50\n",
      "  Training Loss: 0.2408514456315474\n",
      "  Train ROC-AUC Score: 0.9531697798157455\n",
      "  Valid ROC-AUC Score: 0.9506974349465012\n",
      "Epoch 5/50\n",
      "  Training Loss: 0.24221552742852104\n",
      "  Train ROC-AUC Score: 0.953008792339794\n",
      "  Valid ROC-AUC Score: 0.9505371714858483\n",
      "Epoch 6/50\n",
      "  Training Loss: 0.25829860687255857\n",
      "  Train ROC-AUC Score: 0.9534700133482685\n",
      "  Valid ROC-AUC Score: 0.9509673875617091\n",
      "Epoch 7/50\n",
      "  Training Loss: 0.2535620927810669\n",
      "  Train ROC-AUC Score: 0.9542655849701698\n",
      "  Valid ROC-AUC Score: 0.9515202056700118\n",
      "Epoch 8/50\n",
      "  Training Loss: 0.2577223166441306\n",
      "  Train ROC-AUC Score: 0.953594665207975\n",
      "  Valid ROC-AUC Score: 0.9511453561847474\n",
      "Epoch 9/50\n",
      "  Training Loss: 0.25464748299640155\n",
      "  Train ROC-AUC Score: 0.9547283580357357\n",
      "  Valid ROC-AUC Score: 0.9524121469501163\n",
      "Epoch 10/50\n",
      "  Training Loss: 0.25858022769292194\n",
      "  Train ROC-AUC Score: 0.9545386351983001\n",
      "  Valid ROC-AUC Score: 0.9518128317378052\n",
      "Epoch 11/50\n",
      "  Training Loss: 0.2097491979598999\n",
      "  Train ROC-AUC Score: 0.9549521208834171\n",
      "  Valid ROC-AUC Score: 0.9522984163001942\n",
      "Epoch 12/50\n",
      "  Training Loss: 0.26623156491447897\n",
      "  Train ROC-AUC Score: 0.9552165055833719\n",
      "  Valid ROC-AUC Score: 0.952511237211974\n",
      "Epoch 13/50\n",
      "  Training Loss: 0.24148672819137573\n",
      "  Train ROC-AUC Score: 0.9547136474795906\n",
      "  Valid ROC-AUC Score: 0.9523553397670745\n",
      "Epoch 14/50\n",
      "  Training Loss: 0.2381420443134923\n",
      "  Train ROC-AUC Score: 0.9552491300684002\n",
      "  Valid ROC-AUC Score: 0.9524394414213293\n",
      "Epoch 15/50\n",
      "  Training Loss: 0.235704547480533\n",
      "  Train ROC-AUC Score: 0.9556288993852506\n",
      "  Valid ROC-AUC Score: 0.953101284791946\n",
      "Epoch 16/50\n",
      "  Training Loss: 0.2371537102593316\n",
      "  Train ROC-AUC Score: 0.954687929535136\n",
      "  Valid ROC-AUC Score: 0.95199028435131\n",
      "Epoch 17/50\n",
      "  Training Loss: 0.2130676805973053\n",
      "  Train ROC-AUC Score: 0.9555377511717504\n",
      "  Valid ROC-AUC Score: 0.9527239914501398\n",
      "Epoch 18/50\n",
      "  Training Loss: 0.2426081763373481\n",
      "  Train ROC-AUC Score: 0.9559449677780966\n",
      "  Valid ROC-AUC Score: 0.9528660928287588\n",
      "Epoch 19/50\n",
      "  Training Loss: 0.24745908379554749\n",
      "  Train ROC-AUC Score: 0.9559939341775487\n",
      "  Valid ROC-AUC Score: 0.9529375059570977\n",
      "Epoch 20/50\n",
      "  Training Loss: 0.24222971045452615\n",
      "  Train ROC-AUC Score: 0.9562108263687861\n",
      "  Valid ROC-AUC Score: 0.9534063766136274\n",
      "Epoch 21/50\n",
      "  Training Loss: 0.24401473999023438\n",
      "  Train ROC-AUC Score: 0.9562606572157493\n",
      "  Valid ROC-AUC Score: 0.9532460998814496\n",
      "Epoch 22/50\n",
      "  Training Loss: 0.23332121565535263\n",
      "  Train ROC-AUC Score: 0.9562515331768346\n",
      "  Valid ROC-AUC Score: 0.9532063115333549\n",
      "Epoch 23/50\n",
      "  Training Loss: 0.239629940553145\n",
      "  Train ROC-AUC Score: 0.9559771642362862\n",
      "  Valid ROC-AUC Score: 0.9533576963437478\n",
      "Epoch 24/50\n",
      "  Training Loss: 0.19906076788902283\n",
      "  Train ROC-AUC Score: 0.9564719946650159\n",
      "  Valid ROC-AUC Score: 0.9537085335323401\n",
      "Epoch 25/50\n",
      "  Training Loss: 0.2038649171590805\n",
      "  Train ROC-AUC Score: 0.9565339638569219\n",
      "  Valid ROC-AUC Score: 0.9536785300900732\n",
      "Epoch 26/50\n",
      "  Training Loss: 0.23042508761088054\n",
      "  Train ROC-AUC Score: 0.9565742915532489\n",
      "  Valid ROC-AUC Score: 0.9537243683576595\n",
      "Epoch 27/50\n",
      "  Training Loss: 0.24197656458074396\n",
      "  Train ROC-AUC Score: 0.9568363905280577\n",
      "  Valid ROC-AUC Score: 0.954005347082276\n",
      "Epoch 28/50\n",
      "  Training Loss: 0.24441429664348735\n",
      "  Train ROC-AUC Score: 0.9566541147432369\n",
      "  Valid ROC-AUC Score: 0.9538261198762786\n",
      "Epoch 29/50\n",
      "  Training Loss: 0.2325094011094835\n",
      "  Train ROC-AUC Score: 0.9565063416078365\n",
      "  Valid ROC-AUC Score: 0.9532402010045526\n",
      "Epoch 30/50\n",
      "  Training Loss: 0.23185823130053143\n",
      "  Train ROC-AUC Score: 0.9570610778714473\n",
      "  Valid ROC-AUC Score: 0.9542826476985209\n",
      "Epoch 31/50\n",
      "  Training Loss: 0.22353275299072264\n",
      "  Train ROC-AUC Score: 0.9576109597401645\n",
      "  Valid ROC-AUC Score: 0.9544717435473494\n",
      "Epoch 32/50\n",
      "  Training Loss: 0.2177929026739938\n",
      "  Train ROC-AUC Score: 0.9571275781293249\n",
      "  Valid ROC-AUC Score: 0.9538829757215788\n",
      "Epoch 33/50\n",
      "  Training Loss: 0.24618029594421387\n",
      "  Train ROC-AUC Score: 0.9573874559860742\n",
      "  Valid ROC-AUC Score: 0.9541589381257145\n",
      "Epoch 34/50\n",
      "  Training Loss: 0.23567297345116026\n",
      "  Train ROC-AUC Score: 0.957209791801073\n",
      "  Valid ROC-AUC Score: 0.9542037077195455\n",
      "Epoch 35/50\n",
      "  Training Loss: 0.23530147756849015\n",
      "  Train ROC-AUC Score: 0.9573728219527378\n",
      "  Valid ROC-AUC Score: 0.9543246928378519\n",
      "Epoch 36/50\n",
      "  Training Loss: 0.24355575016566686\n",
      "  Train ROC-AUC Score: 0.9574284870426493\n",
      "  Valid ROC-AUC Score: 0.9541519974341011\n",
      "Epoch 37/50\n",
      "  Training Loss: 0.23056561606270926\n",
      "  Train ROC-AUC Score: 0.9577672039759338\n",
      "  Valid ROC-AUC Score: 0.9540685473485285\n",
      "Epoch 38/50\n",
      "  Training Loss: 0.2355456644175004\n",
      "  Train ROC-AUC Score: 0.9573330922730634\n",
      "  Valid ROC-AUC Score: 0.9542148213578218\n",
      "Epoch 39/50\n",
      "  Training Loss: 0.2482336163520813\n",
      "  Train ROC-AUC Score: 0.9576434295555933\n",
      "  Valid ROC-AUC Score: 0.954634994049105\n",
      "Epoch 40/50\n",
      "  Training Loss: 0.2469454178443322\n",
      "  Train ROC-AUC Score: 0.957392392949685\n",
      "  Valid ROC-AUC Score: 0.9544772809331832\n",
      "Epoch 41/50\n",
      "  Training Loss: 0.24433379173278807\n",
      "  Train ROC-AUC Score: 0.957973991928013\n",
      "  Valid ROC-AUC Score: 0.9543258130177641\n",
      "Epoch 42/50\n",
      "  Training Loss: 0.2389860506410952\n",
      "  Train ROC-AUC Score: 0.958030996079031\n",
      "  Valid ROC-AUC Score: 0.9548984499368113\n",
      "Epoch 43/50\n",
      "  Training Loss: 0.24308493558098287\n",
      "  Train ROC-AUC Score: 0.9580576888430662\n",
      "  Valid ROC-AUC Score: 0.9545391098085136\n",
      "Epoch 44/50\n",
      "  Training Loss: 0.23084249729063452\n",
      "  Train ROC-AUC Score: 0.958117109263894\n",
      "  Valid ROC-AUC Score: 0.9546589674791681\n",
      "Epoch 45/50\n",
      "  Training Loss: 0.22624041636784872\n",
      "  Train ROC-AUC Score: 0.9583056390224979\n",
      "  Valid ROC-AUC Score: 0.9548908020625057\n",
      "Epoch 46/50\n",
      "  Training Loss: 0.2552955150604248\n",
      "  Train ROC-AUC Score: 0.9580759800053305\n",
      "  Valid ROC-AUC Score: 0.9547848324108408\n",
      "Epoch 47/50\n",
      "  Training Loss: 0.23039464155832926\n",
      "  Train ROC-AUC Score: 0.9583727924341474\n",
      "  Valid ROC-AUC Score: 0.954660686457651\n",
      "Epoch 48/50\n",
      "  Training Loss: 0.2279172696565327\n",
      "  Train ROC-AUC Score: 0.9580418315991859\n",
      "  Valid ROC-AUC Score: 0.9549181613114005\n",
      "Epoch 49/50\n",
      "  Training Loss: 0.23032668920663688\n",
      "  Train ROC-AUC Score: 0.9578682679488637\n",
      "  Valid ROC-AUC Score: 0.9546977876379114\n",
      "Epoch 50/50\n",
      "  Training Loss: 0.23389952110521722\n",
      "  Train ROC-AUC Score: 0.9582819656907953\n",
      "  Valid ROC-AUC Score: 0.9547231855449967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf6da64b-d894-4a5b-8872-2297c09d05b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC Score: 0.9582819659451538\n",
      "Valid ROC-AUC Score: 0.954723184281042\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "train_score = model.evaluate(train_dataset, [metric])\n",
    "valid_score = model.evaluate(valid_dataset, [metric])\n",
    "print(f\"Train ROC-AUC Score: {train_score['roc_auc_score']}\")\n",
    "print(f\"Valid ROC-AUC Score: {valid_score['roc_auc_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54c08261-88fe-49b2-951b-7a53bfe6f1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC Score: 0.9580634616803781\n",
      "Valid ROC-AUC Score: 0.9570030916444208\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "train_score = model.evaluate(train_dataset, [metric])\n",
    "valid_score = model.evaluate(valid_dataset, [metric])\n",
    "print(f\"Train ROC-AUC Score: {train_score['roc_auc_score']}\")\n",
    "print(f\"Valid ROC-AUC Score: {valid_score['roc_auc_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6331a39-718f-49dd-8769-1893d73729bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_checkpoint(model_dir\u001b[38;5;241m=\u001b[39msave_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save the model\n",
    "save_dir = 'deepchem_model_HSA_30E'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Save the model\n",
    "model.save_checkpoint(model_dir=save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd286e-9fad-4a76-8d92-b7d28431e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b98269-e386-442e-b9e1-c27cd654b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your S3 Bucket and file key\n",
    "bucket = 'kaggle-leash-bio'\n",
    "test_parquet_key = 'test.parquet'\n",
    "test_parquet_location = f's3://{bucket}/{test_parquet_key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a403a34f-c791-44c0-9318-219c0077ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Parquet file\n",
    "df = pd.read_parquet(test_parquet_location, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7e4fa7-f67f-437c-a106-b1ccaa83742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for molecules binding with the HSA protein\n",
    "df_HSA_test = df[df['protein_name'] == 'HSA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb0acad-5802-461c-82ea-4d89463f0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming df_HSA is your DataFrame\n",
    "# df_HSA_test = df_HSA_test.sample(frac=0.001, random_state=42)  # 20% random sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e94567-a485-4ccb-a10d-df600e63f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored successfully.\n"
     ]
    }
   ],
   "source": [
    "# #Loading Model If necessary:\n",
    "\n",
    "# # Restore the model from the checkpoint\n",
    "# model.restore(model_dir=save_dir)\n",
    "# print(\"Model restored successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee99f333-8396-499c-b08d-b00fb65055c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = featurizer.featurize(df_HSA_test['molecule_smiles'].tolist())\n",
    "\n",
    "# Create DeepChem dataset\n",
    "dataset = dc.data.NumpyDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d10a8fa2-8b7a-4353-b9e1-32f6c461077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 19:16:45.343022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "# Predict bindings\n",
    "predictions = model.predict(dataset)\n",
    "\n",
    "# Extract the probability of the positive class (binding)\n",
    "probabilities = predictions[:, 0, 1]  # Assuming the second column corresponds to the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "417c66db-23e4-4626-b30a-d122b7dc7b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>0.002036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834</td>\n",
       "      <td>0.008853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>295246837</td>\n",
       "      <td>0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>295246840</td>\n",
       "      <td>0.076457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>295246843</td>\n",
       "      <td>0.000540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674882</th>\n",
       "      <td>296921712</td>\n",
       "      <td>0.047879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674885</th>\n",
       "      <td>296921715</td>\n",
       "      <td>0.097320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674888</th>\n",
       "      <td>296921718</td>\n",
       "      <td>0.003280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721</td>\n",
       "      <td>0.001998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724</td>\n",
       "      <td>0.002899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>557895 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     binds\n",
       "1        295246831  0.002036\n",
       "4        295246834  0.008853\n",
       "7        295246837  0.000362\n",
       "10       295246840  0.076457\n",
       "13       295246843  0.000540\n",
       "...            ...       ...\n",
       "1674882  296921712  0.047879\n",
       "1674885  296921715  0.097320\n",
       "1674888  296921718  0.003280\n",
       "1674891  296921721  0.001998\n",
       "1674894  296921724  0.002899\n",
       "\n",
       "[557895 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create resulting DataFrame with 'id' and 'binds' columns\n",
    "result_df = pd.DataFrame({\n",
    "    'id': df_HSA_test['id'],\n",
    "    'binds': probabilities\n",
    "})\n",
    "# Display the resulting DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8711afc1-9c3b-4cfd-9fb5-9c308d4e7709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>buildingblock1_smiles</th>\n",
       "      <th>buildingblock2_smiles</th>\n",
       "      <th>buildingblock3_smiles</th>\n",
       "      <th>molecule_smiles</th>\n",
       "      <th>protein_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>295246837</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CC1(C)CCCC1(O)CN</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(NCC2(O)CCCC2(C)C)nc(Nc2ccc(C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>295246840</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>COC(=O)c1cc(Cl)sc1N</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2sc(Cl)c...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>295246843</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CSC1CCC(CN)CC1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(NCC2CCC(SC)CC2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674882</th>\n",
       "      <td>296921712</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1nncs1</td>\n",
       "      <td>Cn1ncc2cc(N)ccc21</td>\n",
       "      <td>Cn1ncc2cc(Nc3nc(Nc4nncs4)nc(N[C@@H](CCCN=[N+]=...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674885</th>\n",
       "      <td>296921715</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1nncs1</td>\n",
       "      <td>NCC1CCC2CC2C1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCC2CCC3CC3C2)nc(Nc2...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674888</th>\n",
       "      <td>296921718</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>COC(=O)c1ccnc(N)c1</td>\n",
       "      <td>COC(=O)c1ccnc(Nc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>COC1CCC(CCN)CC1</td>\n",
       "      <td>COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>557895 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                              buildingblock1_smiles  \\\n",
       "1        295246831    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "4        295246834    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "7        295246837    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "10       295246840    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "13       295246843    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "...            ...                                                ...   \n",
       "1674882  296921712  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674885  296921715  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674888  296921718  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674891  296921721  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674894  296921724  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "\n",
       "        buildingblock2_smiles   buildingblock3_smiles  \\\n",
       "1              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "4              C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n",
       "7              C=Cc1ccc(N)cc1        CC1(C)CCCC1(O)CN   \n",
       "10             C=Cc1ccc(N)cc1     COC(=O)c1cc(Cl)sc1N   \n",
       "13             C=Cc1ccc(N)cc1          CSC1CCC(CN)CC1   \n",
       "...                       ...                     ...   \n",
       "1674882              Nc1nncs1       Cn1ncc2cc(N)ccc21   \n",
       "1674885              Nc1nncs1           NCC1CCC2CC2C1   \n",
       "1674888     Nc1noc2ccc(F)cc12      COC(=O)c1ccnc(N)c1   \n",
       "1674891     Nc1noc2ccc(F)cc12         COC1CCC(CCN)CC1   \n",
       "1674894     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "\n",
       "                                           molecule_smiles protein_name  \n",
       "1        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...          HSA  \n",
       "4        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...          HSA  \n",
       "7        C#CCCC[C@H](Nc1nc(NCC2(O)CCCC2(C)C)nc(Nc2ccc(C...          HSA  \n",
       "10       C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2sc(Cl)c...          HSA  \n",
       "13       C#CCCC[C@H](Nc1nc(NCC2CCC(SC)CC2)nc(Nc2ccc(C=C...          HSA  \n",
       "...                                                    ...          ...  \n",
       "1674882  Cn1ncc2cc(Nc3nc(Nc4nncs4)nc(N[C@@H](CCCN=[N+]=...          HSA  \n",
       "1674885  [N-]=[N+]=NCCC[C@H](Nc1nc(NCC2CCC3CC3C2)nc(Nc2...          HSA  \n",
       "1674888  COC(=O)c1ccnc(Nc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@...          HSA  \n",
       "1674891  COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...          HSA  \n",
       "1674894  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...          HSA  \n",
       "\n",
       "[557895 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_HSA_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a7b1eaa-fee1-40f8-9c64-059f45c109f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the resulting DataFrame to a CSV file\n",
    "result_df.to_csv('HSA_predictions_30E.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "679d577d-928c-460c-b350-238f45306bc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'HSA_predictions_30E.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the prediction CSV files\u001b[39;00m\n\u001b[1;32m      2\u001b[0m she_predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msEH_predictions_30E.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m hsa_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHSA_predictions_30E.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m brd4_predictions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBRD4_predictions_20.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Concatenate the DataFrames\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'HSA_predictions_30E.csv'"
     ]
    }
   ],
   "source": [
    "# Load the prediction CSV files\n",
    "she_predictions = pd.read_csv('sEH_predictions_30E.csv')\n",
    "hsa_predictions = pd.read_csv('HSA_predictions_30E.csv')\n",
    "brd4_predictions = pd.read_csv('BRD4_predictions_20.csv')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "all_predictions = pd.concat([she_predictions, hsa_predictions, brd4_predictions])\n",
    "\n",
    "# Sort by the 'id' column\n",
    "all_predictions_sorted = all_predictions.sort_values(by='id')\n",
    "\n",
    "# Save to a new CSV file\n",
    "all_predictions_sorted.to_csv('final_submission_sEH_HASA_30E.csv', index=False)\n",
    "\n",
    "print(\"final_submission.csv created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d3d57-3b81-4e4a-ae3e-8b5d47a91e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb9661-bcac-401a-b2ce-5a2e1cd41a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a08cc4c-7468-4dd8-80f1-9b4ef764f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Sequence as SequenceCollection\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except ModuleNotFoundError:\n",
    "    raise ImportError('These classes require PyTorch to be installed.')\n",
    "from typing import List, Tuple, Iterable, Optional, Callable, Union, Sequence\n",
    "from deepchem.data import Dataset\n",
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.utils.typing import OneOrMany, ActivationFn\n",
    "from deepchem.models.losses import L2Loss, SoftmaxCrossEntropy\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "import deepchem.models.torch_models.layers as torch_layers\n",
    "from deepchem.utils.pytorch_utils import get_activation\n",
    "\n",
    "\n",
    "class Weave(nn.Module):\n",
    "    \"\"\"\n",
    "    A graph convolutional network(GCN) for either classification or regression.\n",
    "    The network consists of the following sequence of layers:\n",
    "\n",
    "    - Weave feature modules\n",
    "\n",
    "    - Final convolution\n",
    "\n",
    "    - Weave Gather Layer\n",
    "\n",
    "    - A fully connected layer\n",
    "\n",
    "    - A Softmax layer\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> import deepchem as dc\n",
    "    >>> featurizer = dc.feat.WeaveFeaturizer()\n",
    "    >>> X = featurizer([\"C\", \"CC\"])\n",
    "    >>> y = np.array([1, 0])\n",
    "    >>> batch_size = 2\n",
    "    >>> weavemodel = dc.models.WeaveModel(n_tasks=1,n_weave=2, fully_connected_layer_sizes=[2000, 1000],mode=\"classification\",batch_size=batch_size)\n",
    "    >>> atom_feat, pair_feat, pair_split, atom_split, atom_to_pair = weavemodel.compute_features_on_batch(X)\n",
    "    >>> model = Weave(n_tasks=1,n_weave=2,fully_connected_layer_sizes=[2000, 1000],mode=\"classification\")\n",
    "    >>> input_data = [atom_feat, pair_feat, pair_split, atom_split, atom_to_pair]\n",
    "    >>> output = model(input_data)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kearnes, Steven, et al. \"Molecular graph convolutions: moving beyond\n",
    "        fingerprints.\" Journal of computer-aided molecular design 30.8 (2016):\n",
    "        595-608.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_tasks: int,\n",
    "        n_atom_feat: OneOrMany[int] = 75,\n",
    "        n_pair_feat: OneOrMany[int] = 14,\n",
    "        n_hidden: int = 50,\n",
    "        n_graph_feat: int = 128,\n",
    "        n_weave: int = 2,\n",
    "        fully_connected_layer_sizes: List[int] = [2000, 100],\n",
    "        conv_weight_init_stddevs: OneOrMany[float] = 0.03,\n",
    "        weight_init_stddevs: OneOrMany[float] = 0.01,\n",
    "        bias_init_consts: OneOrMany[float] = 0.0,\n",
    "        dropouts: OneOrMany[float] = 0.25,\n",
    "        final_conv_activation_fn=F.tanh,\n",
    "        activation_fns: OneOrMany[ActivationFn] = 'relu',\n",
    "        batch_normalize: bool = True,\n",
    "        gaussian_expand: bool = True,\n",
    "        compress_post_gaussian_expansion: bool = False,\n",
    "        mode: str = \"classification\",\n",
    "        n_classes: int = 2,\n",
    "        batch_size: int = 100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_tasks: int\n",
    "            Number of tasks\n",
    "        n_atom_feat: int, optional (default 75)\n",
    "            Number of features per atom. Note this is 75 by default and should be 78\n",
    "            if chirality is used by `WeaveFeaturizer`.\n",
    "        n_pair_feat: int, optional (default 14)\n",
    "            Number of features per pair of atoms.\n",
    "        n_hidden: int, optional (default 50)\n",
    "            Number of units(convolution depths) in corresponding hidden layer\n",
    "        n_graph_feat: int, optional (default 128)\n",
    "            Number of output features for each molecule(graph)\n",
    "        n_weave: int, optional (default 2)\n",
    "            The number of weave layers in this model.\n",
    "        fully_connected_layer_sizes: list (default `[2000, 100]`)\n",
    "            The size of each dense layer in the network.  The length of\n",
    "            this list determines the number of layers.\n",
    "        conv_weight_init_stddevs: list or float (default 0.03)\n",
    "            The standard deviation of the distribution to use for weight\n",
    "            initialization of each convolutional layer. The length of this lisst\n",
    "            should equal `n_weave`. Alternatively, this may be a single value instead\n",
    "            of a list, in which case the same value is used for each layer.\n",
    "        weight_init_stddevs: list or float (default 0.01)\n",
    "            The standard deviation of the distribution to use for weight\n",
    "            initialization of each fully connected layer.  The length of this list\n",
    "            should equal len(layer_sizes).  Alternatively this may be a single value\n",
    "            instead of a list, in which case the same value is used for every layer.\n",
    "        bias_init_consts: list or float (default 0.0)\n",
    "            The value to initialize the biases in each fully connected layer.  The\n",
    "            length of this list should equal len(layer_sizes).\n",
    "            Alternatively this may be a single value instead of a list, in\n",
    "            which case the same value is used for every layer.\n",
    "        dropouts: list or float (default 0.25)\n",
    "            The dropout probablity to use for each fully connected layer.  The length of this list\n",
    "            should equal len(layer_sizes).  Alternatively this may be a single value\n",
    "            instead of a list, in which case the same value is used for every layer.\n",
    "        final_conv_activation_fn: Optional[ActivationFn] (default `F.tanh`)\n",
    "            The activation funcntion to apply to the final\n",
    "            convolution at the end of the weave convolutions. If `None`, then no\n",
    "            activate is applied (hence linear).\n",
    "        activation_fns: str (default `relu`)\n",
    "            The activation function to apply to each fully connected layer.  The length\n",
    "            of this list should equal len(layer_sizes).  Alternatively this may be a\n",
    "            single value instead of a list, in which case the same value is used for\n",
    "            every layer.\n",
    "        batch_normalize: bool, optional (default True)\n",
    "            If this is turned on, apply batch normalization before applying\n",
    "            activation functions on convolutional and fully connected layers.\n",
    "        gaussian_expand: boolean, optional (default True)\n",
    "            Whether to expand each dimension of atomic features by gaussian\n",
    "            histogram\n",
    "        compress_post_gaussian_expansion: bool, optional (default False)\n",
    "            If True, compress the results of the Gaussian expansion back to the\n",
    "            original dimensions of the input.\n",
    "        mode: str (default \"classification\")\n",
    "            Either \"classification\" or \"regression\" for type of model.\n",
    "        n_classes: int (default 2)\n",
    "            Number of classes to predict (only used in classification mode)\n",
    "        batch_size: int (default 100)\n",
    "            Batch size used by this model for training.\n",
    "        \"\"\"\n",
    "        super(Weave, self).__init__()\n",
    "        if mode not in ['classification', 'regression']:\n",
    "            raise ValueError(\n",
    "                \"mode must be either 'classification' or 'regression'\")\n",
    "\n",
    "        if not isinstance(n_atom_feat, SequenceCollection):\n",
    "            n_atom_feat = [n_atom_feat] * n_weave\n",
    "        if not isinstance(n_pair_feat, SequenceCollection):\n",
    "            n_pair_feat = [n_pair_feat] * n_weave\n",
    "        n_layers = len(fully_connected_layer_sizes)\n",
    "        if not isinstance(conv_weight_init_stddevs, SequenceCollection):\n",
    "            conv_weight_init_stddevs = [conv_weight_init_stddevs] * n_weave\n",
    "        if not isinstance(weight_init_stddevs, SequenceCollection):\n",
    "            weight_init_stddevs = [weight_init_stddevs] * n_layers\n",
    "        if not isinstance(bias_init_consts, SequenceCollection):\n",
    "            bias_init_consts = [bias_init_consts] * n_layers\n",
    "        if not isinstance(dropouts, SequenceCollection):\n",
    "            dropouts = [dropouts] * n_layers\n",
    "        if isinstance(\n",
    "                activation_fns,\n",
    "                str) or not isinstance(activation_fns, SequenceCollection):\n",
    "            activation_fns = [activation_fns] * n_layers\n",
    "\n",
    "        self.n_tasks: int = n_tasks\n",
    "        self.n_atom_feat: OneOrMany[int] = n_atom_feat\n",
    "        self.n_pair_feat: OneOrMany[int] = n_pair_feat\n",
    "        self.n_hidden: int = n_hidden\n",
    "        self.n_graph_feat: int = n_graph_feat\n",
    "        self.mode: str = mode\n",
    "        self.n_classes: int = n_classes\n",
    "        self.n_layers: int = n_layers\n",
    "        self.fully_connected_layer_sizes: List[\n",
    "            int] = fully_connected_layer_sizes\n",
    "        self.weight_init_stddevs: OneOrMany[float] = weight_init_stddevs\n",
    "        self.bias_init_consts: OneOrMany[float] = bias_init_consts\n",
    "        self.dropouts: Sequence[float] = dropouts\n",
    "        self.activation_fns: OneOrMany[ActivationFn] = [\n",
    "            get_activation(i) for i in activation_fns\n",
    "        ]\n",
    "        self.batch_normalize: bool = batch_normalize\n",
    "        self.n_weave: int = n_weave\n",
    "\n",
    "        torch.manual_seed(22)\n",
    "        self.layers: nn.ModuleList = nn.ModuleList()\n",
    "        for ind in range(n_weave):\n",
    "            n_atom: int = self.n_atom_feat[ind]\n",
    "            n_pair: int = self.n_pair_feat[ind]\n",
    "            if ind < n_weave - 1:\n",
    "                n_atom_next: int = self.n_atom_feat[ind + 1]\n",
    "                n_pair_next: int = self.n_pair_feat[ind + 1]\n",
    "            else:\n",
    "                n_atom_next = n_hidden\n",
    "                n_pair_next = n_hidden\n",
    "            weave_layer = torch_layers.WeaveLayer(\n",
    "                n_atom_input_feat=n_atom,\n",
    "                n_pair_input_feat=n_pair,\n",
    "                n_atom_output_feat=n_atom_next,\n",
    "                n_pair_output_feat=n_pair_next,\n",
    "                batch_normalize=batch_normalize)\n",
    "            nn.init.trunc_normal_(weave_layer.W_AA,\n",
    "                                  0,\n",
    "                                  std=conv_weight_init_stddevs[ind])\n",
    "            nn.init.trunc_normal_(weave_layer.W_PA,\n",
    "                                  0,\n",
    "                                  std=conv_weight_init_stddevs[ind])\n",
    "            nn.init.trunc_normal_(weave_layer.W_A,\n",
    "                                  0,\n",
    "                                  std=conv_weight_init_stddevs[ind])\n",
    "            if weave_layer.update_pair:\n",
    "                nn.init.trunc_normal_(weave_layer.W_AP,\n",
    "                                      0,\n",
    "                                      std=conv_weight_init_stddevs[ind])\n",
    "                nn.init.trunc_normal_(weave_layer.W_PP,\n",
    "                                      0,\n",
    "                                      std=conv_weight_init_stddevs[ind])\n",
    "                nn.init.trunc_normal_(weave_layer.W_P,\n",
    "                                      0,\n",
    "                                      std=conv_weight_init_stddevs[ind])\n",
    "            self.layers.append(weave_layer)\n",
    "\n",
    "        self.dense1: nn.Linear = nn.Linear(n_hidden, self.n_graph_feat)\n",
    "        self.dense1_act = final_conv_activation_fn\n",
    "        self.dense1_bn: nn.BatchNorm1d = nn.BatchNorm1d(\n",
    "            num_features=self.n_graph_feat,\n",
    "            eps=1e-3,\n",
    "            momentum=0.99,\n",
    "            affine=True,\n",
    "            track_running_stats=True)\n",
    "\n",
    "        self.weave_gather = torch_layers.WeaveGather(\n",
    "            batch_size,\n",
    "            n_input=self.n_graph_feat,\n",
    "            gaussian_expand=gaussian_expand,\n",
    "            compress_post_gaussian_expansion=compress_post_gaussian_expansion)\n",
    "\n",
    "        if n_layers > 0:\n",
    "            self.layers2: nn.ModuleList = nn.ModuleList()\n",
    "            in_size = self.n_graph_feat * 11\n",
    "            for ind, layer_size, weight_stddev, bias_const, dropout, activation_fn in zip(\n",
    "                [0, 1], fully_connected_layer_sizes, weight_init_stddevs,\n",
    "                    bias_init_consts, dropouts, self.activation_fns):\n",
    "                self.layer: nn.Linear = nn.Linear(in_size, layer_size)\n",
    "                nn.init.trunc_normal_(self.layer.weight, 0, std=weight_stddev)\n",
    "                if self.layer.bias is not None:\n",
    "                    self.layer.bias = nn.Parameter(\n",
    "                        torch.full(self.layer.bias.shape, bias_const))\n",
    "                self.layer.layer_bn = nn.BatchNorm1d(num_features=layer_size,\n",
    "                                                     eps=1e-3,\n",
    "                                                     momentum=0.99,\n",
    "                                                     affine=True,\n",
    "                                                     track_running_stats=True)\n",
    "                self.layer.weight_stddev = weight_stddev\n",
    "                self.layer.bias_const = bias_const\n",
    "                self.layer.dropout = nn.Dropout(dropout)\n",
    "                self.layer.layer_act = activation_fn\n",
    "                self.layers2.append(self.layer)\n",
    "                in_size = layer_size\n",
    "\n",
    "        n_tasks = self.n_tasks\n",
    "        if self.mode == 'classification':\n",
    "            n_classes = self.n_classes\n",
    "            self.layer_2 = nn.Linear(fully_connected_layer_sizes[1],\n",
    "                                     n_tasks * n_classes)\n",
    "\n",
    "        else:\n",
    "            self.layer_2 = nn.Linear(fully_connected_layer_sizes[1], n_tasks)\n",
    "\n",
    "    def forward(self, inputs: OneOrMany[torch.Tensor]) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: OneOrMany[torch.Tensor]\n",
    "            Should contain 5 tensors [atom_features, pair_features, pair_split, atom_split, atom_to_pair]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[torch.Tensor]\n",
    "            Output as per use case : regression/classification\n",
    "        \"\"\"\n",
    "        input1: List[np.ndarray] = [\n",
    "            np.array(inputs[0]),\n",
    "            np.array(inputs[1]),\n",
    "            np.array(inputs[2]),\n",
    "            np.array(inputs[4])\n",
    "        ]\n",
    "        for ind in range(self.n_weave):\n",
    "            weave_layer_ind_A, weave_layer_ind_P = self.layers[ind](input1)\n",
    "            input1 = [\n",
    "                weave_layer_ind_A, weave_layer_ind_P,\n",
    "                np.array(inputs[2]),\n",
    "                np.array(inputs[4])\n",
    "            ]\n",
    "\n",
    "        dense1: torch.Tensor = self.dense1(weave_layer_ind_A)\n",
    "        dense1 = self.dense1_act(dense1)\n",
    "        if self.batch_normalize:\n",
    "            self.dense1_bn.eval()\n",
    "            dense1 = self.dense1_bn(dense1)\n",
    "\n",
    "        weave_gather: torch.Tensor = self.weave_gather([dense1, inputs[3]])\n",
    "        if self.n_layers > 0:\n",
    "            input_layer: torch.Tensor = weave_gather\n",
    "            for ind, dropout in zip([0, 1], self.dropouts):\n",
    "                dense2 = self.layers2[ind]\n",
    "                layer = self.layers2[ind](input_layer)\n",
    "                if dropout > 0.0:\n",
    "                    dense2.dropout.eval()\n",
    "                    layer = dense2.dropout(layer)\n",
    "                if self.batch_normalize:\n",
    "                    dense2.layer_bn.eval()\n",
    "                    layer = dense2.layer_bn(layer)\n",
    "                layer = dense2.layer_act(layer)\n",
    "                input_layer = layer\n",
    "            output: torch.Tensor = input_layer\n",
    "        else:\n",
    "            output = weave_gather\n",
    "\n",
    "        n_tasks = self.n_tasks\n",
    "        if self.mode == 'classification':\n",
    "            n_classes = self.n_classes\n",
    "            logits: torch.Tensor = torch.reshape(self.layer_2(output),\n",
    "                                                 (-1, n_tasks, n_classes))\n",
    "            output = F.softmax(logits, dim=2)\n",
    "            outputs: List[torch.Tensor] = [output, logits]\n",
    "        else:\n",
    "            output = self.layer_2(output)\n",
    "            outputs = [output]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class WeaveModel(TorchModel):\n",
    "    \"\"\"Implements Google-style Weave Graph Convolutions\n",
    "\n",
    "    This model implements the Weave style graph convolutions\n",
    "    from [1]_.\n",
    "\n",
    "    The biggest difference between WeaveModel style convolutions\n",
    "    and GraphConvModel style convolutions is that Weave\n",
    "    convolutions model bond features explicitly. This has the\n",
    "    side effect that it needs to construct a NxN matrix\n",
    "    explicitly to model bond interactions. This may cause\n",
    "    scaling issues, but may possibly allow for better modeling\n",
    "    of subtle bond effects.\n",
    "\n",
    "    Note that [1]_ introduces a whole variety of different architectures for\n",
    "    Weave models. The default settings in this class correspond to the W2N2\n",
    "    variant from [1]_ which is the most commonly used variant..\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    Here's an example of how to fit a `WeaveModel` on a tiny sample dataset.\n",
    "\n",
    "    >>> import numpy as np\n",
    "    >>> import deepchem as dc\n",
    "    >>> featurizer = dc.feat.WeaveFeaturizer()\n",
    "    >>> X = featurizer([\"C\", \"CC\"])\n",
    "    >>> y = np.array([1, 0])\n",
    "    >>> dataset = dc.data.NumpyDataset(X, y)\n",
    "    >>> model = dc.models.WeaveModel(n_tasks=1, n_weave=2, fully_connected_layer_sizes=[2000, 1000], mode=\"classification\")\n",
    "    >>> loss = model.fit(dataset)\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kearnes, Steven, et al. \"Molecular graph convolutions: moving beyond\n",
    "        fingerprints.\" Journal of computer-aided molecular design 30.8 (2016):\n",
    "        595-608.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 n_atom_feat: OneOrMany[int] = 75,\n",
    "                 n_pair_feat: OneOrMany[int] = 14,\n",
    "                 n_hidden: int = 50,\n",
    "                 n_graph_feat: int = 128,\n",
    "                 n_weave: int = 2,\n",
    "                 fully_connected_layer_sizes: List[int] = [2000, 100],\n",
    "                 conv_weight_init_stddevs: OneOrMany[float] = 0.03,\n",
    "                 weight_init_stddevs: OneOrMany[float] = 0.01,\n",
    "                 bias_init_consts: OneOrMany[float] = 0.0,\n",
    "                 weight_decay_penalty: float = 0.0,\n",
    "                 weight_decay_penalty_type: str = \"l2\",\n",
    "                 dropouts: OneOrMany[float] = 0.25,\n",
    "                 final_conv_activation_fn: Optional[ActivationFn] = F.tanh,\n",
    "                 activation_fns: OneOrMany[ActivationFn] = 'relu',\n",
    "                 batch_normalize: bool = True,\n",
    "                 gaussian_expand: bool = True,\n",
    "                 compress_post_gaussian_expansion: bool = False,\n",
    "                 mode: str = \"classification\",\n",
    "                 n_classes: int = 2,\n",
    "                 batch_size: int = 100,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_tasks: int\n",
    "            Number of tasks\n",
    "        n_atom_feat: int, optional (default 75)\n",
    "            Number of features per atom. Note this is 75 by default and should be 78\n",
    "            if chirality is used by `WeaveFeaturizer`.\n",
    "        n_pair_feat: int, optional (default 14)\n",
    "            Number of features per pair of atoms.\n",
    "        n_hidden: int, optional (default 50)\n",
    "            Number of units(convolution depths) in corresponding hidden layer\n",
    "        n_graph_feat: int, optional (default 128)\n",
    "            Number of output features for each molecule(graph)\n",
    "        n_weave: int, optional (default 2)\n",
    "            The number of weave layers in this model.\n",
    "        fully_connected_layer_sizes: list (default `[2000, 100]`)\n",
    "            The size of each dense layer in the network.  The length of\n",
    "            this list determines the number of layers.\n",
    "        conv_weight_init_stddevs: list or float (default 0.03)\n",
    "            The standard deviation of the distribution to use for weight\n",
    "            initialization of each convolutional layer. The length of this lisst\n",
    "            should equal `n_weave`. Alternatively, this may be a single value instead\n",
    "            of a list, in which case the same value is used for each layer.\n",
    "        weight_init_stddevs: list or float (default 0.01)\n",
    "            The standard deviation of the distribution to use for weight\n",
    "            initialization of each fully connected layer.  The length of this list\n",
    "            should equal len(layer_sizes).  Alternatively this may be a single value\n",
    "            instead of a list, in which case the same value is used for every layer.\n",
    "        bias_init_consts: list or float (default 0.0)\n",
    "            The value to initialize the biases in each fully connected layer.  The\n",
    "            length of this list should equal len(layer_sizes).\n",
    "            Alternatively this may be a single value instead of a list, in\n",
    "            which case the same value is used for every layer.\n",
    "        weight_decay_penalty: float (default 0.0)\n",
    "            The magnitude of the weight decay penalty to use\n",
    "        weight_decay_penalty_type: str (default \"l2\")\n",
    "            The type of penalty to use for weight decay, either 'l1' or 'l2'\n",
    "        dropouts: list or float (default 0.25)\n",
    "            The dropout probablity to use for each fully connected layer.  The length of this list\n",
    "            should equal len(layer_sizes).  Alternatively this may be a single value\n",
    "            instead of a list, in which case the same value is used for every layer.\n",
    "        final_conv_activation_fn: Optional[ActivationFn] (default `F.tanh`)\n",
    "            The activation funcntion to apply to the final\n",
    "            convolution at the end of the weave convolutions. If `None`, then no\n",
    "            activate is applied (hence linear).\n",
    "        activation_fns: str (default `relu`)\n",
    "            The activation function to apply to each fully connected layer.  The length\n",
    "            of this list should equal len(layer_sizes).  Alternatively this may be a\n",
    "            single value instead of a list, in which case the same value is used for\n",
    "            every layer.\n",
    "        batch_normalize: bool, optional (default True)\n",
    "            If this is turned on, apply batch normalization before applying\n",
    "            activation functions on convolutional and fully connected layers.\n",
    "        gaussian_expand: boolean, optional (default True)\n",
    "            Whether to expand each dimension of atomic features by gaussian\n",
    "            histogram\n",
    "        compress_post_gaussian_expansion: bool, optional (default False)\n",
    "            If True, compress the results of the Gaussian expansion back to the\n",
    "            original dimensions of the input.\n",
    "        mode: str (default \"classification\")\n",
    "            Either \"classification\" or \"regression\" for type of model.\n",
    "        n_classes: int (default 2)\n",
    "            Number of classes to predict (only used in classification mode)\n",
    "        batch_size: int (default 100)\n",
    "            Batch size used by this model for training.\n",
    "        \"\"\"\n",
    "        self.mode: str = mode\n",
    "        self.model = Weave(\n",
    "            n_tasks=n_tasks,\n",
    "            n_atom_feat=n_atom_feat,\n",
    "            n_pair_feat=n_pair_feat,\n",
    "            n_hidden=n_hidden,\n",
    "            n_graph_feat=n_graph_feat,\n",
    "            n_weave=n_weave,\n",
    "            fully_connected_layer_sizes=fully_connected_layer_sizes,\n",
    "            conv_weight_init_stddevs=conv_weight_init_stddevs,\n",
    "            weight_init_stddevs=weight_init_stddevs,\n",
    "            bias_init_consts=bias_init_consts,\n",
    "            dropouts=dropouts,\n",
    "            final_conv_activation_fn=final_conv_activation_fn,\n",
    "            activation_fns=activation_fns,\n",
    "            batch_normalize=batch_normalize,\n",
    "            gaussian_expand=gaussian_expand,\n",
    "            compress_post_gaussian_expansion=compress_post_gaussian_expansion,\n",
    "            mode=mode,\n",
    "            n_classes=n_classes,\n",
    "            batch_size=batch_size)\n",
    "\n",
    "        if mode not in ['classification', 'regression']:\n",
    "            raise ValueError(\n",
    "                \"mode must be either 'classification' or 'regression'\")\n",
    "\n",
    "        regularization_loss: Optional[Callable]\n",
    "        if weight_decay_penalty != 0.0:\n",
    "            weights = [layer.weight for layer in self.model.layers2]\n",
    "            if weight_decay_penalty_type == 'l1':\n",
    "                regularization_loss = lambda: weight_decay_penalty * torch.sum(  # noqa: E731\n",
    "                    torch.stack([torch.abs(w).sum() for w in weights]))\n",
    "            else:\n",
    "                regularization_loss = lambda: weight_decay_penalty * torch.sum(  # noqa: E731\n",
    "                    torch.stack([torch.square(w).sum() for w in weights]))\n",
    "        else:\n",
    "            regularization_loss = None\n",
    "\n",
    "        loss: Union[SoftmaxCrossEntropy, L2Loss]\n",
    "\n",
    "        if self.mode == 'classification':\n",
    "            output_types = ['prediction', 'loss']\n",
    "            loss = SoftmaxCrossEntropy()\n",
    "        else:\n",
    "            output_types = ['prediction']\n",
    "            loss = L2Loss()\n",
    "\n",
    "        super(WeaveModel,\n",
    "              self).__init__(self.model,\n",
    "                             loss=loss,\n",
    "                             output_types=output_types,\n",
    "                             batch_size=batch_size,\n",
    "                             regularization_loss=regularization_loss,\n",
    "                             **kwargs)\n",
    "\n",
    "    def compute_features_on_batch(self, X_b):\n",
    "        \"\"\"Compute tensors that will be input into the model from featurized representation.\n",
    "\n",
    "        The featurized input to `WeaveModel` is instances of `WeaveMol` created by\n",
    "        `WeaveFeaturizer`. This method converts input `WeaveMol` objects into\n",
    "        tensors used by the Keras implementation to compute `WeaveModel` outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_b: np.ndarray\n",
    "            A numpy array with dtype=object where elements are `WeaveMol` objects.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        atom_feat: np.ndarray\n",
    "            Of shape `(N_atoms, N_atom_feat)`.\n",
    "        pair_feat: np.ndarray\n",
    "            Of shape `(N_pairs, N_pair_feat)`. Note that `N_pairs` will depend on\n",
    "            the number of pairs being considered. If `max_pair_distance` is\n",
    "            `None`, then this will be `N_atoms**2`. Else it will be the number\n",
    "            of pairs within the specifed graph distance.\n",
    "        pair_split: np.ndarray\n",
    "            Of shape `(N_pairs,)`. The i-th entry in this array will tell you the\n",
    "            originating atom for this pair (the \"source\"). Note that pairs are\n",
    "            symmetric so for a pair `(a, b)`, both `a` and `b` will separately be\n",
    "            sources at different points in this array.\n",
    "        atom_split: np.ndarray\n",
    "            Of shape `(N_atoms,)`. The i-th entry in this array will be the molecule\n",
    "            with the i-th atom belongs to.\n",
    "        atom_to_pair: np.ndarray\n",
    "            Of shape `(N_pairs, 2)`. The i-th row in this array will be the array\n",
    "            `[a, b]` if `(a, b)` is a pair to be considered. (Note by symmetry, this\n",
    "            implies some other row will contain `[b, a]`.\n",
    "        \"\"\"\n",
    "        atom_feat = []\n",
    "        pair_feat = []\n",
    "        atom_split = []\n",
    "        atom_to_pair = []\n",
    "        pair_split = []\n",
    "        start = 0\n",
    "        for im, mol in enumerate(X_b):\n",
    "            n_atoms = mol.get_num_atoms()\n",
    "            # pair_edges is of shape (2, N)\n",
    "            pair_edges = mol.get_pair_edges()\n",
    "            # number of atoms in each molecule\n",
    "            atom_split.extend([im] * n_atoms)\n",
    "            # index of pair features\n",
    "            C0, C1 = np.meshgrid(np.arange(n_atoms), np.arange(n_atoms))\n",
    "            atom_to_pair.append(pair_edges.T + start)\n",
    "            # Get starting pair atoms\n",
    "            pair_starts = pair_edges.T[:, 0]\n",
    "            # number of pairs for each atom\n",
    "            pair_split.extend(pair_starts + start)\n",
    "            start = start + n_atoms\n",
    "\n",
    "            # atom features\n",
    "            atom_feat.append(mol.get_atom_features())\n",
    "            # pair features\n",
    "            pair_feat.append(mol.get_pair_features())\n",
    "\n",
    "        return (np.concatenate(atom_feat, axis=0),\n",
    "                np.concatenate(pair_feat, axis=0), np.array(pair_split),\n",
    "                np.array(atom_split), np.concatenate(atom_to_pair, axis=0))\n",
    "\n",
    "    def default_generator(\n",
    "            self,\n",
    "            dataset: Dataset,\n",
    "            epochs: int = 1,\n",
    "            mode: str = 'fit',\n",
    "            deterministic: bool = True,\n",
    "            pad_batches: bool = True) -> Iterable[Tuple[List, List, List]]:\n",
    "        \"\"\"Convert a dataset into the tensors needed for learning.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: `dc.data.Dataset`\n",
    "            Dataset to convert\n",
    "        epochs: int, optional (Default 1)\n",
    "            Number of times to walk over `dataset`\n",
    "        mode: str, optional (Default 'fit')\n",
    "            Ignored in this implementation.\n",
    "        deterministic: bool, optional (Default True)\n",
    "            Whether the dataset should be walked in a deterministic fashion\n",
    "        pad_batches: bool, optional (Default True)\n",
    "            If true, each returned batch will have size `self.batch_size`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Iterator which walks over the batches\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            for (X_b, y_b, w_b,\n",
    "                 ids_b) in dataset.iterbatches(batch_size=self.batch_size,\n",
    "                                               deterministic=deterministic,\n",
    "                                               pad_batches=pad_batches):\n",
    "                if y_b is not None:\n",
    "                    if self.model.mode == 'classification':\n",
    "                        y_b = to_one_hot(y_b.flatten(),\n",
    "                                         self.model.n_classes).reshape(\n",
    "                                             -1, self.model.n_tasks,\n",
    "                                             self.model.n_classes)\n",
    "                inputs = self.compute_features_on_batch(X_b)\n",
    "                yield (inputs, [y_b], [w_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54448ab2-6dcd-4118-ba69-dbc8fc4520ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
